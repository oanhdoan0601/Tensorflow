{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 557438,
     "status": "ok",
     "timestamp": 1553983635997,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     },
     "user_tz": 420
    },
    "id": "_tJKMpm7rAdV",
    "outputId": "694a0e56-9056-4a6e-8366-8f23a78529a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-189e9dfe-9014-4841-ae3a-499f81300ede\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-189e9dfe-9014-4841-ae3a-499f81300ede\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving X.npy to X (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8197,
     "status": "ok",
     "timestamp": 1553984074708,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     },
     "user_tz": 420
    },
    "id": "sy-UJtg0tXYZ",
    "outputId": "db5d03e5-c4cc-4347-bef4-13753c2bf065"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-be637d22-370c-4e5e-b26b-eb0b8168ec73\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-be637d22-370c-4e5e-b26b-eb0b8168ec73\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Y.npy to Y (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4624
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 151029,
     "status": "ok",
     "timestamp": 1553991148215,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     },
     "user_tz": 420
    },
    "id": "3UrfaZuKnfT8",
    "outputId": "a869892e-234c-4d3a-910f-e7d1fb99894d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2078\n",
      "2082\n",
      "(4160, 50, 50, 3)\n",
      "(4160,)\n",
      "max_value -> 255\n",
      "*****   num_batches: 0.22154831886291504  *****\n",
      "NO OF BATCHES: 80\n",
      "*****   NO OF BATCHES: 0.22242164611816406  *****\n",
      "Epoch: 0, Loss: 0.7046, Train_Acc: 0.5962, TEST_Acc: 0.5804, Time: 4.8368\n",
      "Epoch: 1, Loss: 0.6549, Train_Acc: 0.5577, TEST_Acc: 0.6006, Time: 5.4109\n",
      "Epoch: 2, Loss: 0.6544, Train_Acc: 0.5962, TEST_Acc: 0.6352, Time: 5.9690\n",
      "Epoch: 3, Loss: 0.6718, Train_Acc: 0.6154, TEST_Acc: 0.6547, Time: 6.5241\n",
      "Epoch: 4, Loss: 0.6429, Train_Acc: 0.5577, TEST_Acc: 0.6619, Time: 7.0823\n",
      "Epoch: 5, Loss: 0.6143, Train_Acc: 0.5577, TEST_Acc: 0.6633, Time: 7.6344\n",
      "Epoch: 6, Loss: 0.6257, Train_Acc: 0.6731, TEST_Acc: 0.6655, Time: 8.1951\n",
      "Epoch: 7, Loss: 0.6289, Train_Acc: 0.5962, TEST_Acc: 0.6698, Time: 8.7494\n",
      "Epoch: 8, Loss: 0.5971, Train_Acc: 0.6923, TEST_Acc: 0.6633, Time: 9.3068\n",
      "Epoch: 9, Loss: 0.6343, Train_Acc: 0.6538, TEST_Acc: 0.6878, Time: 9.8610\n",
      "Epoch: 10, Loss: 0.6161, Train_Acc: 0.6346, TEST_Acc: 0.6972, Time: 10.4359\n",
      "Epoch: 11, Loss: 0.6194, Train_Acc: 0.6731, TEST_Acc: 0.6907, Time: 10.9914\n",
      "Epoch: 12, Loss: 0.6323, Train_Acc: 0.6731, TEST_Acc: 0.6756, Time: 11.5487\n",
      "Epoch: 13, Loss: 0.6165, Train_Acc: 0.6923, TEST_Acc: 0.7008, Time: 12.0990\n",
      "Epoch: 14, Loss: 0.5921, Train_Acc: 0.7500, TEST_Acc: 0.6936, Time: 12.6630\n",
      "Epoch: 15, Loss: 0.5965, Train_Acc: 0.7115, TEST_Acc: 0.7022, Time: 13.2399\n",
      "Epoch: 16, Loss: 0.5847, Train_Acc: 0.6923, TEST_Acc: 0.6943, Time: 13.7900\n",
      "Epoch: 17, Loss: 0.5853, Train_Acc: 0.6538, TEST_Acc: 0.6929, Time: 14.3459\n",
      "Epoch: 18, Loss: 0.6276, Train_Acc: 0.6346, TEST_Acc: 0.6893, Time: 14.9003\n",
      "Epoch: 19, Loss: 0.6618, Train_Acc: 0.5385, TEST_Acc: 0.6871, Time: 15.4591\n",
      "Epoch: 20, Loss: 0.6025, Train_Acc: 0.6731, TEST_Acc: 0.7001, Time: 16.0142\n",
      "Epoch: 21, Loss: 0.6183, Train_Acc: 0.6731, TEST_Acc: 0.7116, Time: 16.5793\n",
      "Epoch: 22, Loss: 0.6170, Train_Acc: 0.6731, TEST_Acc: 0.7008, Time: 17.1377\n",
      "Epoch: 23, Loss: 0.6654, Train_Acc: 0.5962, TEST_Acc: 0.7022, Time: 17.6989\n",
      "Epoch: 24, Loss: 0.5966, Train_Acc: 0.6923, TEST_Acc: 0.6943, Time: 18.2450\n",
      "Epoch: 25, Loss: 0.6347, Train_Acc: 0.6731, TEST_Acc: 0.7030, Time: 18.7955\n",
      "Epoch: 26, Loss: 0.6090, Train_Acc: 0.6538, TEST_Acc: 0.6864, Time: 19.3383\n",
      "Epoch: 27, Loss: 0.6111, Train_Acc: 0.6923, TEST_Acc: 0.6885, Time: 19.8897\n",
      "Epoch: 28, Loss: 0.5821, Train_Acc: 0.7115, TEST_Acc: 0.7037, Time: 20.4463\n",
      "Epoch: 29, Loss: 0.5694, Train_Acc: 0.7115, TEST_Acc: 0.6972, Time: 20.9940\n",
      "Epoch: 30, Loss: 0.5955, Train_Acc: 0.7500, TEST_Acc: 0.6979, Time: 21.5506\n",
      "Epoch: 31, Loss: 0.5626, Train_Acc: 0.7115, TEST_Acc: 0.6936, Time: 22.1059\n",
      "Epoch: 32, Loss: 0.5602, Train_Acc: 0.7500, TEST_Acc: 0.6965, Time: 22.6600\n",
      "Epoch: 33, Loss: 0.5954, Train_Acc: 0.6923, TEST_Acc: 0.6936, Time: 23.2112\n",
      "Epoch: 34, Loss: 0.5899, Train_Acc: 0.6731, TEST_Acc: 0.6929, Time: 23.7835\n",
      "Epoch: 35, Loss: 0.5717, Train_Acc: 0.7500, TEST_Acc: 0.6900, Time: 24.3355\n",
      "Epoch: 36, Loss: 0.5605, Train_Acc: 0.7500, TEST_Acc: 0.7058, Time: 24.8966\n",
      "Epoch: 37, Loss: 0.5666, Train_Acc: 0.7308, TEST_Acc: 0.6907, Time: 25.4497\n",
      "Epoch: 38, Loss: 0.5455, Train_Acc: 0.7308, TEST_Acc: 0.6907, Time: 26.0070\n",
      "Epoch: 39, Loss: 0.5635, Train_Acc: 0.7500, TEST_Acc: 0.7037, Time: 26.5587\n",
      "Epoch: 40, Loss: 0.5443, Train_Acc: 0.7885, TEST_Acc: 0.7037, Time: 27.1169\n",
      "Epoch: 41, Loss: 0.5690, Train_Acc: 0.7500, TEST_Acc: 0.7030, Time: 27.6772\n",
      "Epoch: 42, Loss: 0.5317, Train_Acc: 0.7885, TEST_Acc: 0.6921, Time: 28.2341\n",
      "Epoch: 43, Loss: 0.5217, Train_Acc: 0.8077, TEST_Acc: 0.7037, Time: 28.7985\n",
      "Epoch: 44, Loss: 0.5191, Train_Acc: 0.7885, TEST_Acc: 0.7037, Time: 29.3486\n",
      "Epoch: 45, Loss: 0.5304, Train_Acc: 0.8077, TEST_Acc: 0.6965, Time: 29.9181\n",
      "Epoch: 46, Loss: 0.5103, Train_Acc: 0.8077, TEST_Acc: 0.7051, Time: 30.4725\n",
      "Epoch: 47, Loss: 0.5126, Train_Acc: 0.7885, TEST_Acc: 0.7022, Time: 31.0324\n",
      "Epoch: 48, Loss: 0.5166, Train_Acc: 0.8269, TEST_Acc: 0.6979, Time: 31.5826\n",
      "Epoch: 49, Loss: 0.4973, Train_Acc: 0.8269, TEST_Acc: 0.6878, Time: 32.1475\n",
      "Epoch: 50, Loss: 0.5077, Train_Acc: 0.8462, TEST_Acc: 0.7058, Time: 32.7045\n",
      "Epoch: 51, Loss: 0.4969, Train_Acc: 0.8462, TEST_Acc: 0.6950, Time: 33.2630\n",
      "Epoch: 52, Loss: 0.5039, Train_Acc: 0.7885, TEST_Acc: 0.6885, Time: 33.8299\n",
      "Epoch: 53, Loss: 0.5039, Train_Acc: 0.8077, TEST_Acc: 0.6936, Time: 34.3898\n",
      "Epoch: 54, Loss: 0.4976, Train_Acc: 0.8462, TEST_Acc: 0.6921, Time: 34.9514\n",
      "Epoch: 55, Loss: 0.4872, Train_Acc: 0.8462, TEST_Acc: 0.6994, Time: 35.5069\n",
      "Epoch: 56, Loss: 0.4944, Train_Acc: 0.8077, TEST_Acc: 0.6828, Time: 36.0720\n",
      "Epoch: 57, Loss: 0.5014, Train_Acc: 0.8077, TEST_Acc: 0.6921, Time: 36.6287\n",
      "Epoch: 58, Loss: 0.4956, Train_Acc: 0.8077, TEST_Acc: 0.6965, Time: 37.1840\n",
      "Epoch: 59, Loss: 0.4884, Train_Acc: 0.8269, TEST_Acc: 0.6878, Time: 37.7311\n",
      "Epoch: 60, Loss: 0.4913, Train_Acc: 0.8269, TEST_Acc: 0.6936, Time: 38.2886\n",
      "Epoch: 61, Loss: 0.4870, Train_Acc: 0.8462, TEST_Acc: 0.7022, Time: 38.8454\n",
      "Epoch: 62, Loss: 0.4856, Train_Acc: 0.8462, TEST_Acc: 0.6878, Time: 39.4007\n",
      "Epoch: 63, Loss: 0.4852, Train_Acc: 0.8462, TEST_Acc: 0.6921, Time: 39.9575\n",
      "Epoch: 64, Loss: 0.4764, Train_Acc: 0.8462, TEST_Acc: 0.7008, Time: 40.5165\n",
      "Epoch: 65, Loss: 0.4900, Train_Acc: 0.8269, TEST_Acc: 0.6994, Time: 41.0751\n",
      "Epoch: 66, Loss: 0.4795, Train_Acc: 0.8462, TEST_Acc: 0.6936, Time: 41.6290\n",
      "Epoch: 67, Loss: 0.4797, Train_Acc: 0.8462, TEST_Acc: 0.6820, Time: 42.1915\n",
      "Epoch: 68, Loss: 0.4717, Train_Acc: 0.8462, TEST_Acc: 0.6893, Time: 42.7361\n",
      "Epoch: 69, Loss: 0.4722, Train_Acc: 0.8654, TEST_Acc: 0.6914, Time: 43.2886\n",
      "Epoch: 70, Loss: 0.4701, Train_Acc: 0.8654, TEST_Acc: 0.6864, Time: 43.8418\n",
      "Epoch: 71, Loss: 0.4685, Train_Acc: 0.8462, TEST_Acc: 0.6950, Time: 44.4060\n",
      "Epoch: 72, Loss: 0.4674, Train_Acc: 0.8462, TEST_Acc: 0.7030, Time: 44.9622\n",
      "Epoch: 73, Loss: 0.4652, Train_Acc: 0.8462, TEST_Acc: 0.6979, Time: 45.5260\n",
      "Epoch: 74, Loss: 0.4584, Train_Acc: 0.8654, TEST_Acc: 0.6972, Time: 46.0799\n",
      "Epoch: 75, Loss: 0.4608, Train_Acc: 0.8654, TEST_Acc: 0.6857, Time: 46.6340\n",
      "Epoch: 76, Loss: 0.4573, Train_Acc: 0.8654, TEST_Acc: 0.6900, Time: 47.1963\n",
      "Epoch: 77, Loss: 0.4596, Train_Acc: 0.8654, TEST_Acc: 0.6907, Time: 47.7518\n",
      "Epoch: 78, Loss: 0.4551, Train_Acc: 0.8654, TEST_Acc: 0.6763, Time: 48.3204\n",
      "Epoch: 79, Loss: 0.4593, Train_Acc: 0.8654, TEST_Acc: 0.6936, Time: 48.8754\n",
      "Epoch: 80, Loss: 0.4598, Train_Acc: 0.8846, TEST_Acc: 0.6849, Time: 49.4443\n",
      "Epoch: 81, Loss: 0.4622, Train_Acc: 0.8654, TEST_Acc: 0.7001, Time: 49.9978\n",
      "Epoch: 82, Loss: 0.4587, Train_Acc: 0.8846, TEST_Acc: 0.6994, Time: 50.5646\n",
      "Epoch: 83, Loss: 0.4574, Train_Acc: 0.8846, TEST_Acc: 0.6835, Time: 51.1207\n",
      "Epoch: 84, Loss: 0.4549, Train_Acc: 0.8846, TEST_Acc: 0.6842, Time: 51.6880\n",
      "Epoch: 85, Loss: 0.4622, Train_Acc: 0.8654, TEST_Acc: 0.6864, Time: 52.2473\n",
      "Epoch: 86, Loss: 0.4608, Train_Acc: 0.8846, TEST_Acc: 0.6806, Time: 52.8140\n",
      "Epoch: 87, Loss: 0.4546, Train_Acc: 0.8846, TEST_Acc: 0.6950, Time: 53.3751\n",
      "Epoch: 88, Loss: 0.4599, Train_Acc: 0.8654, TEST_Acc: 0.6835, Time: 53.9338\n",
      "Epoch: 89, Loss: 0.4563, Train_Acc: 0.8846, TEST_Acc: 0.7015, Time: 54.4928\n",
      "Epoch: 90, Loss: 0.4590, Train_Acc: 0.8654, TEST_Acc: 0.6871, Time: 55.0458\n",
      "Epoch: 91, Loss: 0.4563, Train_Acc: 0.8846, TEST_Acc: 0.6972, Time: 55.6045\n",
      "Epoch: 92, Loss: 0.4533, Train_Acc: 0.8654, TEST_Acc: 0.6907, Time: 56.1572\n",
      "Epoch: 93, Loss: 0.4555, Train_Acc: 0.8846, TEST_Acc: 0.6857, Time: 56.7126\n",
      "Epoch: 94, Loss: 0.4518, Train_Acc: 0.8846, TEST_Acc: 0.6871, Time: 57.2643\n",
      "Epoch: 95, Loss: 0.4597, Train_Acc: 0.8654, TEST_Acc: 0.6994, Time: 57.8252\n",
      "Epoch: 96, Loss: 0.4693, Train_Acc: 0.8654, TEST_Acc: 0.6878, Time: 58.3763\n",
      "Epoch: 97, Loss: 0.4506, Train_Acc: 0.8846, TEST_Acc: 0.6914, Time: 58.9298\n",
      "Epoch: 98, Loss: 0.4512, Train_Acc: 0.8654, TEST_Acc: 0.6900, Time: 59.4832\n",
      "Epoch: 99, Loss: 0.4521, Train_Acc: 0.8654, TEST_Acc: 0.6986, Time: 60.0507\n",
      "Epoch: 100, Loss: 0.4504, Train_Acc: 0.8654, TEST_Acc: 0.6921, Time: 60.6125\n",
      "Epoch: 101, Loss: 0.4483, Train_Acc: 0.8846, TEST_Acc: 0.7044, Time: 61.1652\n",
      "Epoch: 102, Loss: 0.4512, Train_Acc: 0.8846, TEST_Acc: 0.6857, Time: 61.7314\n",
      "Epoch: 103, Loss: 0.4484, Train_Acc: 0.8846, TEST_Acc: 0.6907, Time: 62.2853\n",
      "Epoch: 104, Loss: 0.4500, Train_Acc: 0.8654, TEST_Acc: 0.7073, Time: 62.8514\n",
      "Epoch: 105, Loss: 0.4456, Train_Acc: 0.8846, TEST_Acc: 0.7001, Time: 63.4079\n",
      "Epoch: 106, Loss: 0.4507, Train_Acc: 0.8846, TEST_Acc: 0.6914, Time: 63.9710\n",
      "Epoch: 107, Loss: 0.4480, Train_Acc: 0.8846, TEST_Acc: 0.6871, Time: 64.5312\n",
      "Epoch: 108, Loss: 0.4434, Train_Acc: 0.8846, TEST_Acc: 0.6900, Time: 65.0882\n",
      "Epoch: 109, Loss: 0.4413, Train_Acc: 0.9038, TEST_Acc: 0.6943, Time: 65.6431\n",
      "Epoch: 110, Loss: 0.4491, Train_Acc: 0.8846, TEST_Acc: 0.7001, Time: 66.2079\n",
      "Epoch: 111, Loss: 0.4480, Train_Acc: 0.8846, TEST_Acc: 0.6900, Time: 66.7654\n",
      "Epoch: 112, Loss: 0.4478, Train_Acc: 0.9038, TEST_Acc: 0.6871, Time: 67.3259\n",
      "Epoch: 113, Loss: 0.4460, Train_Acc: 0.9038, TEST_Acc: 0.6965, Time: 67.8901\n",
      "Epoch: 114, Loss: 0.4498, Train_Acc: 0.8462, TEST_Acc: 0.6972, Time: 68.4486\n",
      "Epoch: 115, Loss: 0.4443, Train_Acc: 0.9038, TEST_Acc: 0.6921, Time: 69.0072\n",
      "Epoch: 116, Loss: 0.4430, Train_Acc: 0.8654, TEST_Acc: 0.6972, Time: 69.5659\n",
      "Epoch: 117, Loss: 0.4427, Train_Acc: 0.8846, TEST_Acc: 0.6979, Time: 70.1333\n",
      "Epoch: 118, Loss: 0.4466, Train_Acc: 0.8654, TEST_Acc: 0.6907, Time: 70.6895\n",
      "Epoch: 119, Loss: 0.4456, Train_Acc: 0.8462, TEST_Acc: 0.6950, Time: 71.2500\n",
      "Epoch: 120, Loss: 0.4449, Train_Acc: 0.8846, TEST_Acc: 0.6936, Time: 71.8005\n",
      "Epoch: 121, Loss: 0.4386, Train_Acc: 0.8846, TEST_Acc: 0.6950, Time: 72.3655\n",
      "Epoch: 122, Loss: 0.4487, Train_Acc: 0.8846, TEST_Acc: 0.6907, Time: 72.9253\n",
      "Epoch: 123, Loss: 0.4453, Train_Acc: 0.8654, TEST_Acc: 0.6878, Time: 73.4885\n",
      "Epoch: 124, Loss: 0.4380, Train_Acc: 0.8846, TEST_Acc: 0.6929, Time: 74.0593\n",
      "Epoch: 125, Loss: 0.4406, Train_Acc: 0.8846, TEST_Acc: 0.6857, Time: 74.6145\n",
      "Epoch: 126, Loss: 0.4390, Train_Acc: 0.9038, TEST_Acc: 0.6835, Time: 75.1739\n",
      "Epoch: 127, Loss: 0.4413, Train_Acc: 0.8654, TEST_Acc: 0.6871, Time: 75.7295\n",
      "Epoch: 128, Loss: 0.4305, Train_Acc: 0.9038, TEST_Acc: 0.6900, Time: 76.2864\n",
      "Epoch: 129, Loss: 0.4362, Train_Acc: 0.9038, TEST_Acc: 0.6957, Time: 76.8458\n",
      "Epoch: 130, Loss: 0.4273, Train_Acc: 0.8846, TEST_Acc: 0.6957, Time: 77.4155\n",
      "Epoch: 131, Loss: 0.4315, Train_Acc: 0.8846, TEST_Acc: 0.6871, Time: 77.9789\n",
      "Epoch: 132, Loss: 0.4251, Train_Acc: 0.9038, TEST_Acc: 0.6849, Time: 78.5479\n",
      "Epoch: 133, Loss: 0.4315, Train_Acc: 0.8654, TEST_Acc: 0.6900, Time: 79.1047\n",
      "Epoch: 134, Loss: 0.4298, Train_Acc: 0.8654, TEST_Acc: 0.6828, Time: 79.6701\n",
      "Epoch: 135, Loss: 0.4144, Train_Acc: 0.9231, TEST_Acc: 0.6857, Time: 80.2360\n",
      "Epoch: 136, Loss: 0.4294, Train_Acc: 0.8846, TEST_Acc: 0.6871, Time: 80.7950\n",
      "Epoch: 137, Loss: 0.4148, Train_Acc: 0.9231, TEST_Acc: 0.7001, Time: 81.3579\n",
      "Epoch: 138, Loss: 0.4117, Train_Acc: 0.9231, TEST_Acc: 0.7008, Time: 81.9172\n",
      "Epoch: 139, Loss: 0.4108, Train_Acc: 0.9231, TEST_Acc: 0.6950, Time: 82.4940\n",
      "Epoch: 140, Loss: 0.4109, Train_Acc: 0.9231, TEST_Acc: 0.6907, Time: 83.0542\n",
      "Epoch: 141, Loss: 0.4072, Train_Acc: 0.9231, TEST_Acc: 0.6857, Time: 83.6173\n",
      "Epoch: 142, Loss: 0.4126, Train_Acc: 0.9038, TEST_Acc: 0.6921, Time: 84.1835\n",
      "Epoch: 143, Loss: 0.4106, Train_Acc: 0.9231, TEST_Acc: 0.6979, Time: 84.7491\n",
      "Epoch: 144, Loss: 0.4036, Train_Acc: 0.9231, TEST_Acc: 0.6907, Time: 85.3040\n",
      "Epoch: 145, Loss: 0.4040, Train_Acc: 0.9231, TEST_Acc: 0.6864, Time: 85.8617\n",
      "Epoch: 146, Loss: 0.4162, Train_Acc: 0.9038, TEST_Acc: 0.6965, Time: 86.4257\n",
      "Epoch: 147, Loss: 0.4058, Train_Acc: 0.9038, TEST_Acc: 0.6756, Time: 86.9776\n",
      "Epoch: 148, Loss: 0.4082, Train_Acc: 0.9231, TEST_Acc: 0.6849, Time: 87.5400\n",
      "Epoch: 149, Loss: 0.4058, Train_Acc: 0.9231, TEST_Acc: 0.6849, Time: 88.0939\n",
      "Epoch: 150, Loss: 0.4047, Train_Acc: 0.9231, TEST_Acc: 0.6806, Time: 88.6556\n",
      "Epoch: 151, Loss: 0.3975, Train_Acc: 0.9231, TEST_Acc: 0.6784, Time: 89.2059\n",
      "Epoch: 152, Loss: 0.4103, Train_Acc: 0.9038, TEST_Acc: 0.6835, Time: 89.7709\n",
      "Epoch: 153, Loss: 0.3992, Train_Acc: 0.9231, TEST_Acc: 0.6756, Time: 90.3233\n",
      "Epoch: 154, Loss: 0.3984, Train_Acc: 0.9231, TEST_Acc: 0.6734, Time: 90.8807\n",
      "Epoch: 155, Loss: 0.4088, Train_Acc: 0.9231, TEST_Acc: 0.6864, Time: 91.4323\n",
      "Epoch: 156, Loss: 0.4130, Train_Acc: 0.9231, TEST_Acc: 0.6712, Time: 91.9918\n",
      "Epoch: 157, Loss: 0.4027, Train_Acc: 0.9231, TEST_Acc: 0.6986, Time: 92.5541\n",
      "Epoch: 158, Loss: 0.4043, Train_Acc: 0.9231, TEST_Acc: 0.6792, Time: 93.1058\n",
      "Epoch: 159, Loss: 0.4020, Train_Acc: 0.9231, TEST_Acc: 0.6820, Time: 93.6665\n",
      "Epoch: 160, Loss: 0.3960, Train_Acc: 0.9231, TEST_Acc: 0.6842, Time: 94.2251\n",
      "Epoch: 161, Loss: 0.3975, Train_Acc: 0.9231, TEST_Acc: 0.6857, Time: 94.7887\n",
      "Epoch: 162, Loss: 0.4034, Train_Acc: 0.9231, TEST_Acc: 0.6770, Time: 95.3410\n",
      "Epoch: 163, Loss: 0.3986, Train_Acc: 0.9231, TEST_Acc: 0.6885, Time: 95.9081\n",
      "Epoch: 164, Loss: 0.4071, Train_Acc: 0.9231, TEST_Acc: 0.6878, Time: 96.4624\n",
      "Epoch: 165, Loss: 0.4066, Train_Acc: 0.9231, TEST_Acc: 0.6914, Time: 97.0315\n",
      "Epoch: 166, Loss: 0.3919, Train_Acc: 0.9231, TEST_Acc: 0.6849, Time: 97.5875\n",
      "Epoch: 167, Loss: 0.3975, Train_Acc: 0.9231, TEST_Acc: 0.6799, Time: 98.1520\n",
      "Epoch: 168, Loss: 0.4027, Train_Acc: 0.9231, TEST_Acc: 0.6885, Time: 98.7176\n",
      "Epoch: 169, Loss: 0.3954, Train_Acc: 0.9231, TEST_Acc: 0.6662, Time: 99.2756\n",
      "Epoch: 170, Loss: 0.4007, Train_Acc: 0.9231, TEST_Acc: 0.6777, Time: 99.8418\n",
      "Epoch: 171, Loss: 0.3995, Train_Acc: 0.9231, TEST_Acc: 0.6885, Time: 100.3959\n",
      "Epoch: 172, Loss: 0.4007, Train_Acc: 0.9231, TEST_Acc: 0.6748, Time: 100.9608\n",
      "Epoch: 173, Loss: 0.3925, Train_Acc: 0.9231, TEST_Acc: 0.6770, Time: 101.5129\n",
      "Epoch: 174, Loss: 0.3953, Train_Acc: 0.9231, TEST_Acc: 0.6828, Time: 102.0748\n",
      "Epoch: 175, Loss: 0.4046, Train_Acc: 0.9231, TEST_Acc: 0.6914, Time: 102.6288\n",
      "Epoch: 176, Loss: 0.3955, Train_Acc: 0.9231, TEST_Acc: 0.6871, Time: 103.1882\n",
      "Epoch: 177, Loss: 0.3939, Train_Acc: 0.9231, TEST_Acc: 0.6792, Time: 103.7404\n",
      "Epoch: 178, Loss: 0.3967, Train_Acc: 0.9231, TEST_Acc: 0.6907, Time: 104.3011\n",
      "Epoch: 179, Loss: 0.3970, Train_Acc: 0.9231, TEST_Acc: 0.6842, Time: 104.8529\n",
      "Epoch: 180, Loss: 0.3994, Train_Acc: 0.9231, TEST_Acc: 0.6849, Time: 105.4172\n",
      "Epoch: 181, Loss: 0.3942, Train_Acc: 0.9231, TEST_Acc: 0.6777, Time: 105.9761\n",
      "Epoch: 182, Loss: 0.3886, Train_Acc: 0.9231, TEST_Acc: 0.6871, Time: 106.5366\n",
      "Epoch: 183, Loss: 0.4010, Train_Acc: 0.9231, TEST_Acc: 0.6878, Time: 107.1040\n",
      "Epoch: 184, Loss: 0.3966, Train_Acc: 0.9231, TEST_Acc: 0.6799, Time: 107.6710\n",
      "Epoch: 185, Loss: 0.3971, Train_Acc: 0.9231, TEST_Acc: 0.6857, Time: 108.2302\n",
      "Epoch: 186, Loss: 0.4022, Train_Acc: 0.9231, TEST_Acc: 0.6792, Time: 108.7816\n",
      "Epoch: 187, Loss: 0.3968, Train_Acc: 0.9231, TEST_Acc: 0.6777, Time: 109.3455\n",
      "Epoch: 188, Loss: 0.4019, Train_Acc: 0.9038, TEST_Acc: 0.6806, Time: 109.9038\n",
      "Epoch: 189, Loss: 0.4012, Train_Acc: 0.9231, TEST_Acc: 0.6748, Time: 110.4824\n",
      "Epoch: 190, Loss: 0.4085, Train_Acc: 0.9231, TEST_Acc: 0.6957, Time: 111.0372\n",
      "Epoch: 191, Loss: 0.4119, Train_Acc: 0.9038, TEST_Acc: 0.6835, Time: 111.6117\n",
      "Epoch: 192, Loss: 0.4066, Train_Acc: 0.9231, TEST_Acc: 0.7001, Time: 112.1817\n",
      "Epoch: 193, Loss: 0.3995, Train_Acc: 0.9423, TEST_Acc: 0.6893, Time: 112.7450\n",
      "Epoch: 194, Loss: 0.4015, Train_Acc: 0.9231, TEST_Acc: 0.6871, Time: 113.3141\n",
      "Epoch: 195, Loss: 0.4077, Train_Acc: 0.9231, TEST_Acc: 0.6820, Time: 113.8737\n",
      "Epoch: 196, Loss: 0.4135, Train_Acc: 0.9038, TEST_Acc: 0.6842, Time: 114.4541\n",
      "Epoch: 197, Loss: 0.4062, Train_Acc: 0.9231, TEST_Acc: 0.6957, Time: 115.0105\n",
      "Epoch: 198, Loss: 0.4082, Train_Acc: 0.9231, TEST_Acc: 0.6799, Time: 115.5781\n",
      "Epoch: 199, Loss: 0.4054, Train_Acc: 0.9423, TEST_Acc: 0.6813, Time: 116.1356\n",
      "Epoch: 200, Loss: 0.4094, Train_Acc: 0.9423, TEST_Acc: 0.6705, Time: 116.6998\n",
      "Epoch: 201, Loss: 0.4132, Train_Acc: 0.9231, TEST_Acc: 0.6676, Time: 117.2549\n",
      "Epoch: 202, Loss: 0.4142, Train_Acc: 0.9231, TEST_Acc: 0.6799, Time: 117.8101\n",
      "Epoch: 203, Loss: 0.4064, Train_Acc: 0.9423, TEST_Acc: 0.6683, Time: 118.3678\n",
      "Epoch: 204, Loss: 0.4433, Train_Acc: 0.8846, TEST_Acc: 0.6626, Time: 118.9240\n",
      "Epoch: 205, Loss: 0.4139, Train_Acc: 0.9231, TEST_Acc: 0.6727, Time: 119.4779\n",
      "Epoch: 206, Loss: 0.4127, Train_Acc: 0.9231, TEST_Acc: 0.6763, Time: 120.0285\n",
      "Epoch: 207, Loss: 0.4147, Train_Acc: 0.9231, TEST_Acc: 0.6799, Time: 120.5889\n",
      "Epoch: 208, Loss: 0.4093, Train_Acc: 0.9231, TEST_Acc: 0.6748, Time: 121.1440\n",
      "Epoch: 209, Loss: 0.4030, Train_Acc: 0.9231, TEST_Acc: 0.6763, Time: 121.6997\n",
      "Epoch: 210, Loss: 0.4058, Train_Acc: 0.9231, TEST_Acc: 0.6633, Time: 122.2551\n",
      "Epoch: 211, Loss: 0.3921, Train_Acc: 0.9423, TEST_Acc: 0.6741, Time: 122.8130\n",
      "Epoch: 212, Loss: 0.3898, Train_Acc: 0.9423, TEST_Acc: 0.6626, Time: 123.3676\n",
      "Epoch: 213, Loss: 0.3894, Train_Acc: 0.9423, TEST_Acc: 0.6799, Time: 123.9289\n",
      "Epoch: 214, Loss: 0.3825, Train_Acc: 0.9423, TEST_Acc: 0.6784, Time: 124.4842\n",
      "Epoch: 215, Loss: 0.3900, Train_Acc: 0.9423, TEST_Acc: 0.6734, Time: 125.0405\n",
      "Epoch: 216, Loss: 0.3857, Train_Acc: 0.9423, TEST_Acc: 0.6792, Time: 125.6036\n",
      "Epoch: 217, Loss: 0.3842, Train_Acc: 0.9423, TEST_Acc: 0.6784, Time: 126.1610\n",
      "Epoch: 218, Loss: 0.3881, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 126.7229\n",
      "Epoch: 219, Loss: 0.3891, Train_Acc: 0.9423, TEST_Acc: 0.6763, Time: 127.2752\n",
      "Epoch: 220, Loss: 0.3834, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 127.8389\n",
      "Epoch: 221, Loss: 0.3833, Train_Acc: 0.9423, TEST_Acc: 0.6691, Time: 128.3954\n",
      "Epoch: 222, Loss: 0.3824, Train_Acc: 0.9423, TEST_Acc: 0.6748, Time: 128.9621\n",
      "Epoch: 223, Loss: 0.3809, Train_Acc: 0.9423, TEST_Acc: 0.6792, Time: 129.5209\n",
      "Epoch: 224, Loss: 0.3800, Train_Acc: 0.9423, TEST_Acc: 0.6792, Time: 130.0822\n",
      "Epoch: 225, Loss: 0.3781, Train_Acc: 0.9423, TEST_Acc: 0.6857, Time: 130.6368\n",
      "Epoch: 226, Loss: 0.3781, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 131.2047\n",
      "Epoch: 227, Loss: 0.3784, Train_Acc: 0.9423, TEST_Acc: 0.6835, Time: 131.7650\n",
      "Epoch: 228, Loss: 0.3780, Train_Acc: 0.9423, TEST_Acc: 0.6857, Time: 132.3221\n",
      "Epoch: 229, Loss: 0.3778, Train_Acc: 0.9423, TEST_Acc: 0.6835, Time: 132.8896\n",
      "Epoch: 230, Loss: 0.3764, Train_Acc: 0.9423, TEST_Acc: 0.6842, Time: 133.4441\n",
      "Epoch: 231, Loss: 0.3784, Train_Acc: 0.9423, TEST_Acc: 0.6864, Time: 134.0093\n",
      "Epoch: 232, Loss: 0.3781, Train_Acc: 0.9423, TEST_Acc: 0.6705, Time: 134.5615\n",
      "Epoch: 233, Loss: 0.3791, Train_Acc: 0.9423, TEST_Acc: 0.6691, Time: 135.1233\n",
      "Epoch: 234, Loss: 0.3784, Train_Acc: 0.9423, TEST_Acc: 0.6734, Time: 135.6860\n",
      "Epoch: 235, Loss: 0.3827, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 136.2557\n",
      "Epoch: 236, Loss: 0.3765, Train_Acc: 0.9423, TEST_Acc: 0.6691, Time: 136.8075\n",
      "Epoch: 237, Loss: 0.3776, Train_Acc: 0.9423, TEST_Acc: 0.6741, Time: 137.3654\n",
      "Epoch: 238, Loss: 0.3760, Train_Acc: 0.9423, TEST_Acc: 0.6698, Time: 137.9269\n",
      "Epoch: 239, Loss: 0.3764, Train_Acc: 0.9423, TEST_Acc: 0.6799, Time: 138.4781\n",
      "Epoch: 240, Loss: 0.3849, Train_Acc: 0.9423, TEST_Acc: 0.6748, Time: 139.0371\n",
      "Epoch: 241, Loss: 0.3875, Train_Acc: 0.9423, TEST_Acc: 0.6784, Time: 139.5910\n",
      "Epoch: 242, Loss: 0.4440, Train_Acc: 0.8462, TEST_Acc: 0.6770, Time: 140.1491\n",
      "Epoch: 243, Loss: 0.3902, Train_Acc: 0.9231, TEST_Acc: 0.6741, Time: 140.7028\n",
      "Epoch: 244, Loss: 0.3880, Train_Acc: 0.9423, TEST_Acc: 0.6799, Time: 141.2583\n",
      "Epoch: 245, Loss: 0.3815, Train_Acc: 0.9423, TEST_Acc: 0.6820, Time: 141.8097\n",
      "Epoch: 246, Loss: 0.3798, Train_Acc: 0.9423, TEST_Acc: 0.6806, Time: 142.3718\n",
      "Epoch: 247, Loss: 0.3861, Train_Acc: 0.9423, TEST_Acc: 0.6878, Time: 142.9263\n",
      "Epoch: 248, Loss: 0.3789, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 143.4878\n",
      "Epoch: 249, Loss: 0.3706, Train_Acc: 0.9423, TEST_Acc: 0.6871, Time: 144.0454\n",
      "Epoch: 250, Loss: 0.3743, Train_Acc: 0.9423, TEST_Acc: 0.6770, Time: 144.6177\n",
      "Epoch: 251, Loss: 0.3739, Train_Acc: 0.9423, TEST_Acc: 0.6820, Time: 145.1829\n",
      "Epoch: 252, Loss: 0.3715, Train_Acc: 0.9423, TEST_Acc: 0.6813, Time: 145.7398\n",
      "Epoch: 253, Loss: 0.3758, Train_Acc: 0.9423, TEST_Acc: 0.6741, Time: 146.3049\n",
      "Epoch: 254, Loss: 0.3721, Train_Acc: 0.9423, TEST_Acc: 0.6806, Time: 146.8637\n",
      "Epoch: 255, Loss: 0.3702, Train_Acc: 0.9423, TEST_Acc: 0.6748, Time: 147.4273\n",
      "Epoch: 256, Loss: 0.3671, Train_Acc: 0.9423, TEST_Acc: 0.6792, Time: 147.9829\n",
      "Epoch: 257, Loss: 0.3692, Train_Acc: 0.9423, TEST_Acc: 0.6727, Time: 148.5465\n",
      "Epoch: 258, Loss: 0.3672, Train_Acc: 0.9423, TEST_Acc: 0.6669, Time: 149.1002\n",
      "Epoch: 259, Loss: 0.3676, Train_Acc: 0.9423, TEST_Acc: 0.6849, Time: 149.6631\n",
      "Epoch: 260, Loss: 0.3572, Train_Acc: 0.9615, TEST_Acc: 0.6806, Time: 150.2190\n",
      "                  Testing accuracy: 0.6806, Time: 150.2564\n",
      "*****   END: 150.3108503818512  *****\n"
     ]
    }
   ],
   "source": [
    "# Breast Histology Images\n",
    "# Classify IDC vs non IDC images\n",
    "#\n",
    "# https://www.kaggle.com/simjeg/lymphoma-subtype-classification-fl-vs-cll\n",
    "#\n",
    "# This dataset consists of 5547 breast histology images of size 50 x 50 x 3,\n",
    "# The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images.\n",
    "#\n",
    "# Perceptron\n",
    "#\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def print_time(message, start_time):\n",
    "    print(\"*****   \" + message + \": {}  *****\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "def get_batch(all_data, all_labels, batch_size=16):\n",
    "    # // means divide and result is integer, / returns float\n",
    "    # rows_data = len(all_data) // batch_size\n",
    "    # rows_labels = len(all_labels) // batch_size\n",
    "\n",
    "    rtn_data = all_data.reshape(-1, batch_size, int(50*50*3))\n",
    "    rnt_labels = all_labels.reshape(-1, batch_size, 2)\n",
    "\n",
    "    return (rtn_data, rnt_labels)\n",
    "\n",
    "\n",
    "def multiperceptron(x):\n",
    "    # tf.nn.sigmoid() 1/(1+e^-x)\n",
    "    # x[16, 3072], l1[16,256], l2[16, 256], out[16, 10]\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['h3']), biases['b3']))\n",
    "    outl = tf.nn.sigmoid(tf.add(tf.matmul(l3, weights['out']), biases['out']))\n",
    "    return outl\n",
    "\n",
    "#####################################################################################################\n",
    "#\n",
    "#   Main\n",
    "#\n",
    "#\n",
    "######################################################################################################\n",
    "START_TIME = time.time()\n",
    "path = r'C:\\Users\\gtune\\OneDrive\\document_usk\\UCSC\\02_SecondQuater\\Deep_Learning_and_Artificial_Intelligence_with_TensorFlow\\Homework\\Final_Project\\data'\n",
    "\n",
    "X_data = np.load(\"X.npy\")\n",
    "Y_truth = np.load(\"Y.npy\")\n",
    "\n",
    "# print(X_data[0:10])\n",
    "# print(X_data.shape)\n",
    "# print(Y_truth[0:100])\n",
    "\n",
    "# print(np.array(np.where(Y_truth == 1)))\n",
    "\n",
    "train_data, test_data, train_Y, test_Y = \\\n",
    "    train_test_split(X_data, Y_truth, test_size=0.25, random_state=3)\n",
    "\n",
    "print(len(np.array(np.where(train_Y == 1)).ravel()))\n",
    "print(len(np.array(np.where(train_Y == 0)).ravel()))\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_Y.shape)\n",
    "#print(train_Y[0:50])\n",
    "\n",
    "# architecture hyper-parameter\n",
    "num_datapoints = len(train_data)\n",
    "learning_rate = 0.0001   # 0.001\n",
    "n_epoch = 1000\n",
    "batch_size = 52  # divisor of 4160 : 1, 2, 4, 5, 8, 10, 13, 16, 20, 26, 32, 40, 52, 64, 65, 80, 104, 130, 160, 208, 260, 320, 416, 520, 832, 1040, 2080, 4160\n",
    "\n",
    "n_input = 50*50*3\n",
    "n_classes = 2   # IDC and non-IDC\n",
    "dropout = 0.75\n",
    "\n",
    "# Normalization\n",
    "max_value = np.max(train_data)\n",
    "print('max_value -> {}'.format(max_value))\n",
    "\n",
    "train_data = np.array(train_data/max_value, dtype=np.float32)\n",
    "\n",
    "# data and Y(Label) reshaped\n",
    "onehot_train_Y = np.full((len(train_Y), n_classes), 0)\n",
    "onehot_train_Y[np.arange(0, len(train_Y)), train_Y] = 1\n",
    "train_data, onehot_train_Y = get_batch(train_data, onehot_train_Y, batch_size)\n",
    "\n",
    "test_data = test_data.reshape(-1, n_input)\n",
    "onehot_test_Y = np.full((len(test_Y), n_classes), 0)\n",
    "onehot_test_Y[np.arange(0, len(test_Y)), test_Y] = 1\n",
    "\n",
    "\n",
    "num_batches = num_datapoints // batch_size  # 10  # num_datapoints // batch_size\n",
    "\n",
    "print_time(\"num_batches\", START_TIME)\n",
    "\n",
    "# data_x, labels_y = get_batch(X_train, onehot_labels, batch_size)\n",
    "\n",
    "print(\"NO OF BATCHES:\", num_batches)\n",
    "print_time(\"NO OF BATCHES\", START_TIME)\n",
    "\n",
    "\n",
    "# tensorflow placeholder\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 512\n",
    "n_hidden3 = 256\n",
    "\n",
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden2, n_hidden3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden3, n_classes]))\n",
    "}\n",
    "\n",
    "# b1=[512], b2=[256], b3=[128], out=[10]\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "# Create the Model\n",
    "model = multiperceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate the model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initialing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorflow Session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(num_batches):\n",
    "            # for _ in range(1):\n",
    "\n",
    "            batch_x = train_data[i]\n",
    "            batch_y = onehot_train_Y[i]\n",
    "\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        # varidate after every epoch\n",
    "        batch_x = train_data[0]\n",
    "        batch_y = onehot_train_Y[0]\n",
    "\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:batch_x, Y:batch_y, keep_prob:1.0})\n",
    "\n",
    "        test_accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Train_Acc: %0.4f, TEST_Acc: %0.4f, Time: %0.4f\" % (epoch, losscalc, accuracycalc, test_accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "        # when train accuracy is over 95%, program end\n",
    "        if accuracycalc >= 0.95:\n",
    "            break\n",
    "\n",
    "    # display the accuracy of using testing data\n",
    "    accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "    print(\"                  Testing accuracy: %0.4f, Time: %0.4f\" % (accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "print_time(\"END\", START_TIME)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled0.ipynb",
   "provenance": [
    {
     "file_id": "1B8mwfFJWeY9JRcfZEImsBnbJ2h3-m076",
     "timestamp": 1553991371481
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
