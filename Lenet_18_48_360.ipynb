{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_tJKMpm7rAdV",
    "colab_type": "code",
    "outputId": "694a0e56-9056-4a6e-8366-8f23a78529a0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553983635997E12,
     "user_tz": 420.0,
     "elapsed": 557438.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 79.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-189e9dfe-9014-4841-ae3a-499f81300ede\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-189e9dfe-9014-4841-ae3a-499f81300ede\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving X.npy to X (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sy-UJtg0tXYZ",
    "colab_type": "code",
    "outputId": "db5d03e5-c4cc-4347-bef4-13753c2bf065",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553984074708E12,
     "user_tz": 420.0,
     "elapsed": 8197.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 79.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-be637d22-370c-4e5e-b26b-eb0b8168ec73\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-be637d22-370c-4e5e-b26b-eb0b8168ec73\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Y.npy to Y (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3UrfaZuKnfT8",
    "colab_type": "code",
    "outputId": "bfd54292-ebed-4920-b7f3-f39e05242869",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553986912598E12,
     "user_tz": 420.0,
     "elapsed": 230900.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4811.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2078\n",
      "2082\n",
      "(4160, 50, 50, 3)\n",
      "(4160,)\n",
      "max_value -> 255\n",
      "*****   num_batches: 0.2580108642578125  *****\n",
      "NO OF BATCHES: 80\n",
      "*****   NO OF BATCHES: 0.25908398628234863  *****\n",
      "conv1.get_shape():  (?, 50, 50, 18)\n",
      "conv1.get_shape() after maxpool :  (?, 25, 25, 18)\n",
      "conv2.get_shape():  (?, 25, 25, 48)\n",
      "conv2.get_shape() after maxpool :  (?, 13, 13, 48)\n",
      "Epoch: 0, Loss: 3168.0627, Train_Acc: 0.5769, TEST_Acc: 0.5876, Time: 4.6269\n",
      "Epoch: 1, Loss: 2640.0820, Train_Acc: 0.6154, TEST_Acc: 0.5941, Time: 5.4745\n",
      "Epoch: 2, Loss: 2372.3643, Train_Acc: 0.6154, TEST_Acc: 0.6056, Time: 6.3243\n",
      "Epoch: 3, Loss: 2203.3882, Train_Acc: 0.5962, TEST_Acc: 0.6179, Time: 7.1637\n",
      "Epoch: 4, Loss: 2098.2505, Train_Acc: 0.5962, TEST_Acc: 0.6301, Time: 8.0062\n",
      "Epoch: 5, Loss: 2050.9756, Train_Acc: 0.5769, TEST_Acc: 0.6352, Time: 8.8451\n",
      "Epoch: 6, Loss: 1952.7042, Train_Acc: 0.5769, TEST_Acc: 0.6410, Time: 9.6920\n",
      "Epoch: 7, Loss: 1890.2374, Train_Acc: 0.5577, TEST_Acc: 0.6489, Time: 10.5350\n",
      "Epoch: 8, Loss: 1848.7249, Train_Acc: 0.5577, TEST_Acc: 0.6474, Time: 11.3793\n",
      "Epoch: 9, Loss: 1784.0763, Train_Acc: 0.6154, TEST_Acc: 0.6460, Time: 12.2259\n",
      "Epoch: 10, Loss: 1727.8982, Train_Acc: 0.5962, TEST_Acc: 0.6438, Time: 13.0692\n",
      "Epoch: 11, Loss: 1730.1713, Train_Acc: 0.6346, TEST_Acc: 0.6438, Time: 13.9045\n",
      "Epoch: 12, Loss: 1740.9120, Train_Acc: 0.6346, TEST_Acc: 0.6431, Time: 14.7432\n",
      "Epoch: 13, Loss: 1710.5759, Train_Acc: 0.6346, TEST_Acc: 0.6431, Time: 15.5833\n",
      "Epoch: 14, Loss: 1672.9240, Train_Acc: 0.6538, TEST_Acc: 0.6460, Time: 16.4344\n",
      "Epoch: 15, Loss: 1644.1261, Train_Acc: 0.6538, TEST_Acc: 0.6467, Time: 17.2808\n",
      "Epoch: 16, Loss: 1618.9630, Train_Acc: 0.6346, TEST_Acc: 0.6446, Time: 18.1168\n",
      "Epoch: 17, Loss: 1572.6067, Train_Acc: 0.6346, TEST_Acc: 0.6474, Time: 18.9590\n",
      "Epoch: 18, Loss: 1503.2866, Train_Acc: 0.6346, TEST_Acc: 0.6431, Time: 19.7988\n",
      "Epoch: 19, Loss: 1458.8347, Train_Acc: 0.6538, TEST_Acc: 0.6453, Time: 20.6386\n",
      "Epoch: 20, Loss: 1382.4773, Train_Acc: 0.6731, TEST_Acc: 0.6417, Time: 21.4852\n",
      "Epoch: 21, Loss: 1343.3976, Train_Acc: 0.6538, TEST_Acc: 0.6438, Time: 22.3305\n",
      "Epoch: 22, Loss: 1332.0851, Train_Acc: 0.6538, TEST_Acc: 0.6467, Time: 23.1716\n",
      "Epoch: 23, Loss: 1282.5697, Train_Acc: 0.6538, TEST_Acc: 0.6467, Time: 24.0152\n",
      "Epoch: 24, Loss: 1294.9894, Train_Acc: 0.5962, TEST_Acc: 0.6539, Time: 24.8589\n",
      "Epoch: 25, Loss: 1210.2054, Train_Acc: 0.6346, TEST_Acc: 0.6547, Time: 25.7019\n",
      "Epoch: 26, Loss: 1165.5248, Train_Acc: 0.6731, TEST_Acc: 0.6532, Time: 26.5501\n",
      "Epoch: 27, Loss: 1145.1835, Train_Acc: 0.6538, TEST_Acc: 0.6510, Time: 27.3892\n",
      "Epoch: 28, Loss: 1117.3123, Train_Acc: 0.6154, TEST_Acc: 0.6554, Time: 28.2297\n",
      "Epoch: 29, Loss: 1075.9792, Train_Acc: 0.6731, TEST_Acc: 0.6547, Time: 29.0684\n",
      "Epoch: 30, Loss: 1065.2007, Train_Acc: 0.6346, TEST_Acc: 0.6561, Time: 29.9093\n",
      "Epoch: 31, Loss: 1048.3085, Train_Acc: 0.6346, TEST_Acc: 0.6590, Time: 30.7510\n",
      "Epoch: 32, Loss: 997.6208, Train_Acc: 0.6538, TEST_Acc: 0.6547, Time: 31.5898\n",
      "Epoch: 33, Loss: 968.0482, Train_Acc: 0.6923, TEST_Acc: 0.6510, Time: 32.4265\n",
      "Epoch: 34, Loss: 981.0629, Train_Acc: 0.6923, TEST_Acc: 0.6496, Time: 33.2651\n",
      "Epoch: 35, Loss: 979.2987, Train_Acc: 0.6923, TEST_Acc: 0.6496, Time: 34.1102\n",
      "Epoch: 36, Loss: 953.8499, Train_Acc: 0.6731, TEST_Acc: 0.6460, Time: 34.9523\n",
      "Epoch: 37, Loss: 916.7308, Train_Acc: 0.6538, TEST_Acc: 0.6510, Time: 35.7953\n",
      "Epoch: 38, Loss: 921.2772, Train_Acc: 0.6538, TEST_Acc: 0.6496, Time: 36.6341\n",
      "Epoch: 39, Loss: 869.2081, Train_Acc: 0.6346, TEST_Acc: 0.6539, Time: 37.4803\n",
      "Epoch: 40, Loss: 850.7284, Train_Acc: 0.6923, TEST_Acc: 0.6510, Time: 38.3272\n",
      "Epoch: 41, Loss: 821.2504, Train_Acc: 0.6154, TEST_Acc: 0.6604, Time: 39.1688\n",
      "Epoch: 42, Loss: 799.6405, Train_Acc: 0.6731, TEST_Acc: 0.6604, Time: 40.0130\n",
      "Epoch: 43, Loss: 799.5961, Train_Acc: 0.6731, TEST_Acc: 0.6611, Time: 40.8548\n",
      "Epoch: 44, Loss: 827.0245, Train_Acc: 0.6923, TEST_Acc: 0.6640, Time: 41.6943\n",
      "Epoch: 45, Loss: 806.4636, Train_Acc: 0.6731, TEST_Acc: 0.6669, Time: 42.5419\n",
      "Epoch: 46, Loss: 898.0092, Train_Acc: 0.6731, TEST_Acc: 0.6676, Time: 43.3881\n",
      "Epoch: 47, Loss: 923.5964, Train_Acc: 0.6731, TEST_Acc: 0.6705, Time: 44.2307\n",
      "Epoch: 48, Loss: 1048.6915, Train_Acc: 0.6346, TEST_Acc: 0.6712, Time: 45.0753\n",
      "Epoch: 49, Loss: 775.0762, Train_Acc: 0.6923, TEST_Acc: 0.6676, Time: 45.9146\n",
      "Epoch: 50, Loss: 712.5036, Train_Acc: 0.6923, TEST_Acc: 0.6590, Time: 46.7644\n",
      "Epoch: 51, Loss: 653.1786, Train_Acc: 0.7500, TEST_Acc: 0.6655, Time: 47.6077\n",
      "Epoch: 52, Loss: 640.1082, Train_Acc: 0.7308, TEST_Acc: 0.6532, Time: 48.4571\n",
      "Epoch: 53, Loss: 634.4609, Train_Acc: 0.7308, TEST_Acc: 0.6510, Time: 49.3004\n",
      "Epoch: 54, Loss: 613.6062, Train_Acc: 0.7308, TEST_Acc: 0.6503, Time: 50.1387\n",
      "Epoch: 55, Loss: 589.9109, Train_Acc: 0.7500, TEST_Acc: 0.6547, Time: 50.9832\n",
      "Epoch: 56, Loss: 602.8834, Train_Acc: 0.7115, TEST_Acc: 0.6431, Time: 51.8319\n",
      "Epoch: 57, Loss: 588.5815, Train_Acc: 0.7500, TEST_Acc: 0.6453, Time: 52.6793\n",
      "Epoch: 58, Loss: 571.9410, Train_Acc: 0.7115, TEST_Acc: 0.6424, Time: 53.5317\n",
      "Epoch: 59, Loss: 564.6932, Train_Acc: 0.7308, TEST_Acc: 0.6388, Time: 54.3773\n",
      "Epoch: 60, Loss: 551.8746, Train_Acc: 0.7500, TEST_Acc: 0.6402, Time: 55.2191\n",
      "Epoch: 61, Loss: 559.1501, Train_Acc: 0.7308, TEST_Acc: 0.6381, Time: 56.0665\n",
      "Epoch: 62, Loss: 564.1780, Train_Acc: 0.7500, TEST_Acc: 0.6287, Time: 56.9143\n",
      "Epoch: 63, Loss: 523.7003, Train_Acc: 0.7500, TEST_Acc: 0.6337, Time: 57.7641\n",
      "Epoch: 64, Loss: 509.7736, Train_Acc: 0.7500, TEST_Acc: 0.6431, Time: 58.6076\n",
      "Epoch: 65, Loss: 501.2964, Train_Acc: 0.7500, TEST_Acc: 0.6431, Time: 59.4495\n",
      "Epoch: 66, Loss: 492.4381, Train_Acc: 0.7692, TEST_Acc: 0.6417, Time: 60.2965\n",
      "Epoch: 67, Loss: 497.3610, Train_Acc: 0.7500, TEST_Acc: 0.6460, Time: 61.1351\n",
      "Epoch: 68, Loss: 490.8598, Train_Acc: 0.7500, TEST_Acc: 0.6424, Time: 61.9783\n",
      "Epoch: 69, Loss: 472.7560, Train_Acc: 0.7500, TEST_Acc: 0.6251, Time: 62.8261\n",
      "Epoch: 70, Loss: 461.7349, Train_Acc: 0.7692, TEST_Acc: 0.6366, Time: 63.6693\n",
      "Epoch: 71, Loss: 476.0180, Train_Acc: 0.7692, TEST_Acc: 0.6482, Time: 64.5060\n",
      "Epoch: 72, Loss: 448.6567, Train_Acc: 0.7500, TEST_Acc: 0.6395, Time: 65.3480\n",
      "Epoch: 73, Loss: 416.3069, Train_Acc: 0.7885, TEST_Acc: 0.6410, Time: 66.1921\n",
      "Epoch: 74, Loss: 405.9268, Train_Acc: 0.7692, TEST_Acc: 0.6265, Time: 67.0393\n",
      "Epoch: 75, Loss: 418.6544, Train_Acc: 0.7885, TEST_Acc: 0.6150, Time: 67.8930\n",
      "Epoch: 76, Loss: 406.2244, Train_Acc: 0.8077, TEST_Acc: 0.6107, Time: 68.7392\n",
      "Epoch: 77, Loss: 427.8558, Train_Acc: 0.7885, TEST_Acc: 0.6006, Time: 69.5791\n",
      "Epoch: 78, Loss: 419.2328, Train_Acc: 0.7885, TEST_Acc: 0.5984, Time: 70.4258\n",
      "Epoch: 79, Loss: 371.8628, Train_Acc: 0.7692, TEST_Acc: 0.6208, Time: 71.2758\n",
      "Epoch: 80, Loss: 347.4625, Train_Acc: 0.7692, TEST_Acc: 0.6215, Time: 72.1242\n",
      "Epoch: 81, Loss: 322.0612, Train_Acc: 0.8462, TEST_Acc: 0.6179, Time: 72.9672\n",
      "Epoch: 82, Loss: 395.9218, Train_Acc: 0.7692, TEST_Acc: 0.6337, Time: 73.8027\n",
      "Epoch: 83, Loss: 512.5756, Train_Acc: 0.7500, TEST_Acc: 0.6482, Time: 74.6492\n",
      "Epoch: 84, Loss: 689.1432, Train_Acc: 0.7115, TEST_Acc: 0.6561, Time: 75.4915\n",
      "Epoch: 85, Loss: 689.5355, Train_Acc: 0.7308, TEST_Acc: 0.6575, Time: 76.3299\n",
      "Epoch: 86, Loss: 661.5902, Train_Acc: 0.6923, TEST_Acc: 0.6583, Time: 77.1752\n",
      "Epoch: 87, Loss: 574.3895, Train_Acc: 0.7500, TEST_Acc: 0.6547, Time: 78.0197\n",
      "Epoch: 88, Loss: 536.4661, Train_Acc: 0.7500, TEST_Acc: 0.6532, Time: 78.8546\n",
      "Epoch: 89, Loss: 490.6139, Train_Acc: 0.7692, TEST_Acc: 0.6503, Time: 79.6942\n",
      "Epoch: 90, Loss: 475.8609, Train_Acc: 0.7885, TEST_Acc: 0.6431, Time: 80.5340\n",
      "Epoch: 91, Loss: 444.3824, Train_Acc: 0.7885, TEST_Acc: 0.6366, Time: 81.3791\n",
      "Epoch: 92, Loss: 410.2338, Train_Acc: 0.8077, TEST_Acc: 0.6337, Time: 82.2272\n",
      "Epoch: 93, Loss: 420.6571, Train_Acc: 0.8077, TEST_Acc: 0.6373, Time: 83.0635\n",
      "Epoch: 94, Loss: 565.6090, Train_Acc: 0.7500, TEST_Acc: 0.6539, Time: 83.9091\n",
      "Epoch: 95, Loss: 617.0071, Train_Acc: 0.7500, TEST_Acc: 0.6482, Time: 84.7612\n",
      "Epoch: 96, Loss: 564.6000, Train_Acc: 0.7692, TEST_Acc: 0.6489, Time: 85.6023\n",
      "Epoch: 97, Loss: 478.3928, Train_Acc: 0.7885, TEST_Acc: 0.6388, Time: 86.4510\n",
      "Epoch: 98, Loss: 624.4418, Train_Acc: 0.7115, TEST_Acc: 0.6467, Time: 87.2989\n",
      "Epoch: 99, Loss: 652.6562, Train_Acc: 0.7308, TEST_Acc: 0.6482, Time: 88.1412\n",
      "Epoch: 100, Loss: 711.2646, Train_Acc: 0.6731, TEST_Acc: 0.6518, Time: 88.9808\n",
      "Epoch: 101, Loss: 323.4564, Train_Acc: 0.8269, TEST_Acc: 0.5984, Time: 89.8204\n",
      "Epoch: 102, Loss: 332.0607, Train_Acc: 0.8269, TEST_Acc: 0.5991, Time: 90.6606\n",
      "Epoch: 103, Loss: 326.3828, Train_Acc: 0.8269, TEST_Acc: 0.5912, Time: 91.5032\n",
      "Epoch: 104, Loss: 337.0665, Train_Acc: 0.8654, TEST_Acc: 0.5999, Time: 92.3393\n",
      "Epoch: 105, Loss: 337.7143, Train_Acc: 0.8462, TEST_Acc: 0.5926, Time: 93.1778\n",
      "Epoch: 106, Loss: 317.6588, Train_Acc: 0.8269, TEST_Acc: 0.5818, Time: 94.0172\n",
      "Epoch: 107, Loss: 315.9207, Train_Acc: 0.8077, TEST_Acc: 0.5789, Time: 94.8604\n",
      "Epoch: 108, Loss: 305.9392, Train_Acc: 0.8077, TEST_Acc: 0.5789, Time: 95.7057\n",
      "Epoch: 109, Loss: 289.0504, Train_Acc: 0.8462, TEST_Acc: 0.5797, Time: 96.5477\n",
      "Epoch: 110, Loss: 281.9399, Train_Acc: 0.8654, TEST_Acc: 0.5818, Time: 97.3876\n",
      "Epoch: 111, Loss: 285.7682, Train_Acc: 0.8462, TEST_Acc: 0.5833, Time: 98.2334\n",
      "Epoch: 112, Loss: 337.3019, Train_Acc: 0.8269, TEST_Acc: 0.5905, Time: 99.0790\n",
      "Epoch: 113, Loss: 304.9044, Train_Acc: 0.8269, TEST_Acc: 0.5847, Time: 99.9241\n",
      "Epoch: 114, Loss: 310.0603, Train_Acc: 0.8462, TEST_Acc: 0.5869, Time: 100.7689\n",
      "Epoch: 115, Loss: 415.6688, Train_Acc: 0.7692, TEST_Acc: 0.6056, Time: 101.6133\n",
      "Epoch: 116, Loss: 389.3243, Train_Acc: 0.7885, TEST_Acc: 0.6027, Time: 102.4586\n",
      "Epoch: 117, Loss: 381.3838, Train_Acc: 0.7885, TEST_Acc: 0.6020, Time: 103.2974\n",
      "Epoch: 118, Loss: 364.8041, Train_Acc: 0.7885, TEST_Acc: 0.6020, Time: 104.1431\n",
      "Epoch: 119, Loss: 344.2838, Train_Acc: 0.8077, TEST_Acc: 0.5963, Time: 104.9883\n",
      "Epoch: 120, Loss: 387.9039, Train_Acc: 0.7885, TEST_Acc: 0.6013, Time: 105.8353\n",
      "Epoch: 121, Loss: 413.2188, Train_Acc: 0.7692, TEST_Acc: 0.6027, Time: 106.6752\n",
      "Epoch: 122, Loss: 373.6727, Train_Acc: 0.7885, TEST_Acc: 0.6013, Time: 107.5173\n",
      "Epoch: 123, Loss: 306.8244, Train_Acc: 0.8846, TEST_Acc: 0.5912, Time: 108.3608\n",
      "Epoch: 124, Loss: 279.4809, Train_Acc: 0.9038, TEST_Acc: 0.5840, Time: 109.2012\n",
      "Epoch: 125, Loss: 265.9101, Train_Acc: 0.8462, TEST_Acc: 0.5811, Time: 110.0442\n",
      "Epoch: 126, Loss: 269.3034, Train_Acc: 0.9038, TEST_Acc: 0.5847, Time: 110.8886\n",
      "Epoch: 127, Loss: 386.9026, Train_Acc: 0.7500, TEST_Acc: 0.5970, Time: 111.7322\n",
      "Epoch: 128, Loss: 320.3721, Train_Acc: 0.8269, TEST_Acc: 0.5869, Time: 112.5768\n",
      "Epoch: 129, Loss: 283.2133, Train_Acc: 0.8654, TEST_Acc: 0.5869, Time: 113.4232\n",
      "Epoch: 130, Loss: 344.9449, Train_Acc: 0.7885, TEST_Acc: 0.5955, Time: 114.2679\n",
      "Epoch: 131, Loss: 327.2894, Train_Acc: 0.8077, TEST_Acc: 0.5883, Time: 115.1118\n",
      "Epoch: 132, Loss: 423.1303, Train_Acc: 0.7308, TEST_Acc: 0.5977, Time: 115.9499\n",
      "Epoch: 133, Loss: 435.6534, Train_Acc: 0.7308, TEST_Acc: 0.5991, Time: 116.7921\n",
      "Epoch: 134, Loss: 327.1744, Train_Acc: 0.8462, TEST_Acc: 0.5905, Time: 117.6356\n",
      "Epoch: 135, Loss: 273.5598, Train_Acc: 0.8846, TEST_Acc: 0.5804, Time: 118.4799\n",
      "Epoch: 136, Loss: 248.5263, Train_Acc: 0.8462, TEST_Acc: 0.5696, Time: 119.3220\n",
      "Epoch: 137, Loss: 229.6580, Train_Acc: 0.9038, TEST_Acc: 0.5652, Time: 120.1640\n",
      "Epoch: 138, Loss: 228.7423, Train_Acc: 0.9038, TEST_Acc: 0.5710, Time: 121.0047\n",
      "Epoch: 139, Loss: 262.4109, Train_Acc: 0.8462, TEST_Acc: 0.5689, Time: 121.8453\n",
      "Epoch: 140, Loss: 441.0555, Train_Acc: 0.7308, TEST_Acc: 0.5905, Time: 122.6882\n",
      "Epoch: 141, Loss: 790.6325, Train_Acc: 0.6731, TEST_Acc: 0.6157, Time: 123.5243\n",
      "Epoch: 142, Loss: 828.7266, Train_Acc: 0.6923, TEST_Acc: 0.6244, Time: 124.3694\n",
      "Epoch: 143, Loss: 1056.0044, Train_Acc: 0.6346, TEST_Acc: 0.6359, Time: 125.2034\n",
      "Epoch: 144, Loss: 920.3528, Train_Acc: 0.6538, TEST_Acc: 0.6352, Time: 126.0422\n",
      "Epoch: 145, Loss: 637.4439, Train_Acc: 0.7115, TEST_Acc: 0.6071, Time: 126.8845\n",
      "Epoch: 146, Loss: 412.7315, Train_Acc: 0.8077, TEST_Acc: 0.5854, Time: 127.7270\n",
      "Epoch: 147, Loss: 519.4745, Train_Acc: 0.7115, TEST_Acc: 0.5898, Time: 128.5738\n",
      "Epoch: 148, Loss: 502.8046, Train_Acc: 0.7115, TEST_Acc: 0.5898, Time: 129.4210\n",
      "Epoch: 149, Loss: 471.6632, Train_Acc: 0.7308, TEST_Acc: 0.5890, Time: 130.2597\n",
      "Epoch: 150, Loss: 488.7857, Train_Acc: 0.7308, TEST_Acc: 0.5898, Time: 131.1100\n",
      "Epoch: 151, Loss: 479.7674, Train_Acc: 0.7115, TEST_Acc: 0.5905, Time: 131.9572\n",
      "Epoch: 152, Loss: 520.0098, Train_Acc: 0.7115, TEST_Acc: 0.5999, Time: 132.8081\n",
      "Epoch: 153, Loss: 553.1612, Train_Acc: 0.7115, TEST_Acc: 0.5984, Time: 133.6533\n",
      "Epoch: 154, Loss: 484.7264, Train_Acc: 0.7308, TEST_Acc: 0.5926, Time: 134.4900\n",
      "Epoch: 155, Loss: 448.3093, Train_Acc: 0.7308, TEST_Acc: 0.5941, Time: 135.3283\n",
      "Epoch: 156, Loss: 431.8086, Train_Acc: 0.7308, TEST_Acc: 0.5862, Time: 136.1692\n",
      "Epoch: 157, Loss: 407.1109, Train_Acc: 0.7692, TEST_Acc: 0.5789, Time: 137.0173\n",
      "Epoch: 158, Loss: 352.6782, Train_Acc: 0.8077, TEST_Acc: 0.5768, Time: 137.8572\n",
      "Epoch: 159, Loss: 376.0449, Train_Acc: 0.8077, TEST_Acc: 0.5826, Time: 138.6933\n",
      "Epoch: 160, Loss: 420.9337, Train_Acc: 0.6923, TEST_Acc: 0.5898, Time: 139.5262\n",
      "Epoch: 161, Loss: 480.4088, Train_Acc: 0.7115, TEST_Acc: 0.5977, Time: 140.3686\n",
      "Epoch: 162, Loss: 474.3624, Train_Acc: 0.7308, TEST_Acc: 0.5948, Time: 141.2101\n",
      "Epoch: 163, Loss: 485.3032, Train_Acc: 0.7308, TEST_Acc: 0.5991, Time: 142.0523\n",
      "Epoch: 164, Loss: 539.1586, Train_Acc: 0.7115, TEST_Acc: 0.6035, Time: 142.8977\n",
      "Epoch: 165, Loss: 514.1087, Train_Acc: 0.7500, TEST_Acc: 0.6020, Time: 143.7394\n",
      "Epoch: 166, Loss: 419.6385, Train_Acc: 0.7692, TEST_Acc: 0.5984, Time: 144.5918\n",
      "Epoch: 167, Loss: 433.2078, Train_Acc: 0.7500, TEST_Acc: 0.6035, Time: 145.4366\n",
      "Epoch: 168, Loss: 300.3171, Train_Acc: 0.8462, TEST_Acc: 0.5948, Time: 146.2872\n",
      "Epoch: 169, Loss: 246.5977, Train_Acc: 0.8462, TEST_Acc: 0.5768, Time: 147.1350\n",
      "Epoch: 170, Loss: 216.6914, Train_Acc: 0.8846, TEST_Acc: 0.5717, Time: 147.9806\n",
      "Epoch: 171, Loss: 196.3876, Train_Acc: 0.9038, TEST_Acc: 0.5602, Time: 148.8171\n",
      "Epoch: 172, Loss: 201.2181, Train_Acc: 0.9038, TEST_Acc: 0.5624, Time: 149.6523\n",
      "Epoch: 173, Loss: 205.3190, Train_Acc: 0.9038, TEST_Acc: 0.5689, Time: 150.4943\n",
      "Epoch: 174, Loss: 200.6838, Train_Acc: 0.9231, TEST_Acc: 0.5609, Time: 151.3376\n",
      "Epoch: 175, Loss: 189.3338, Train_Acc: 0.9038, TEST_Acc: 0.5595, Time: 152.1772\n",
      "Epoch: 176, Loss: 188.8951, Train_Acc: 0.9423, TEST_Acc: 0.5602, Time: 153.0132\n",
      "Epoch: 177, Loss: 193.2760, Train_Acc: 0.9231, TEST_Acc: 0.5689, Time: 153.8520\n",
      "Epoch: 178, Loss: 195.8943, Train_Acc: 0.9231, TEST_Acc: 0.5703, Time: 154.6911\n",
      "Epoch: 179, Loss: 202.5695, Train_Acc: 0.9038, TEST_Acc: 0.5753, Time: 155.5337\n",
      "Epoch: 180, Loss: 195.9836, Train_Acc: 0.9038, TEST_Acc: 0.5717, Time: 156.3825\n",
      "Epoch: 181, Loss: 220.3435, Train_Acc: 0.8462, TEST_Acc: 0.5739, Time: 157.2174\n",
      "Epoch: 182, Loss: 191.9459, Train_Acc: 0.9038, TEST_Acc: 0.5717, Time: 158.0574\n",
      "Epoch: 183, Loss: 210.4225, Train_Acc: 0.8846, TEST_Acc: 0.5761, Time: 158.9007\n",
      "Epoch: 184, Loss: 183.7843, Train_Acc: 0.9231, TEST_Acc: 0.5746, Time: 159.7512\n",
      "Epoch: 185, Loss: 205.1234, Train_Acc: 0.8846, TEST_Acc: 0.5746, Time: 160.6006\n",
      "Epoch: 186, Loss: 181.0133, Train_Acc: 0.9231, TEST_Acc: 0.5725, Time: 161.4452\n",
      "Epoch: 187, Loss: 184.6550, Train_Acc: 0.9038, TEST_Acc: 0.5725, Time: 162.2843\n",
      "Epoch: 188, Loss: 182.0176, Train_Acc: 0.9038, TEST_Acc: 0.5732, Time: 163.1266\n",
      "Epoch: 189, Loss: 256.5162, Train_Acc: 0.8077, TEST_Acc: 0.5826, Time: 163.9741\n",
      "Epoch: 190, Loss: 208.0717, Train_Acc: 0.8846, TEST_Acc: 0.5811, Time: 164.8140\n",
      "Epoch: 191, Loss: 179.0219, Train_Acc: 0.9231, TEST_Acc: 0.5681, Time: 165.6602\n",
      "Epoch: 192, Loss: 249.5700, Train_Acc: 0.8077, TEST_Acc: 0.5883, Time: 166.4931\n",
      "Epoch: 193, Loss: 241.3028, Train_Acc: 0.8269, TEST_Acc: 0.5905, Time: 167.3374\n",
      "Epoch: 194, Loss: 376.9152, Train_Acc: 0.7500, TEST_Acc: 0.6042, Time: 168.1738\n",
      "Epoch: 195, Loss: 303.7297, Train_Acc: 0.8269, TEST_Acc: 0.6013, Time: 169.0116\n",
      "Epoch: 196, Loss: 352.0385, Train_Acc: 0.7885, TEST_Acc: 0.6071, Time: 169.8553\n",
      "Epoch: 197, Loss: 415.8489, Train_Acc: 0.7500, TEST_Acc: 0.6121, Time: 170.6974\n",
      "Epoch: 198, Loss: 340.7404, Train_Acc: 0.8269, TEST_Acc: 0.6071, Time: 171.5424\n",
      "Epoch: 199, Loss: 228.1800, Train_Acc: 0.8462, TEST_Acc: 0.5926, Time: 172.3828\n",
      "Epoch: 200, Loss: 161.9314, Train_Acc: 0.9231, TEST_Acc: 0.5703, Time: 173.2309\n",
      "Epoch: 201, Loss: 178.4542, Train_Acc: 0.9423, TEST_Acc: 0.5746, Time: 174.0752\n",
      "Epoch: 202, Loss: 174.7549, Train_Acc: 0.9423, TEST_Acc: 0.5782, Time: 174.9183\n",
      "Epoch: 203, Loss: 181.2070, Train_Acc: 0.9231, TEST_Acc: 0.5782, Time: 175.7638\n",
      "Epoch: 204, Loss: 173.6521, Train_Acc: 0.9423, TEST_Acc: 0.5717, Time: 176.6049\n",
      "Epoch: 205, Loss: 176.1198, Train_Acc: 0.9423, TEST_Acc: 0.5717, Time: 177.4482\n",
      "Epoch: 206, Loss: 170.1750, Train_Acc: 0.9423, TEST_Acc: 0.5624, Time: 178.2967\n",
      "Epoch: 207, Loss: 193.5102, Train_Acc: 0.9038, TEST_Acc: 0.5789, Time: 179.1398\n",
      "Epoch: 208, Loss: 197.5747, Train_Acc: 0.8654, TEST_Acc: 0.5804, Time: 179.9807\n",
      "Epoch: 209, Loss: 173.3265, Train_Acc: 0.9423, TEST_Acc: 0.5811, Time: 180.8291\n",
      "Epoch: 210, Loss: 319.2933, Train_Acc: 0.7885, TEST_Acc: 0.5991, Time: 181.6728\n",
      "Epoch: 211, Loss: 317.4744, Train_Acc: 0.7885, TEST_Acc: 0.5977, Time: 182.5156\n",
      "Epoch: 212, Loss: 275.4018, Train_Acc: 0.8269, TEST_Acc: 0.5999, Time: 183.3583\n",
      "Epoch: 213, Loss: 188.3088, Train_Acc: 0.8846, TEST_Acc: 0.5847, Time: 184.1972\n",
      "Epoch: 214, Loss: 245.7446, Train_Acc: 0.8462, TEST_Acc: 0.5934, Time: 185.0334\n",
      "Epoch: 215, Loss: 272.4596, Train_Acc: 0.7692, TEST_Acc: 0.5941, Time: 185.8705\n",
      "Epoch: 216, Loss: 213.4509, Train_Acc: 0.8269, TEST_Acc: 0.5941, Time: 186.7098\n",
      "Epoch: 217, Loss: 170.5423, Train_Acc: 0.9231, TEST_Acc: 0.5797, Time: 187.5428\n",
      "Epoch: 218, Loss: 200.5465, Train_Acc: 0.8462, TEST_Acc: 0.5826, Time: 188.3811\n",
      "Epoch: 219, Loss: 196.4058, Train_Acc: 0.8077, TEST_Acc: 0.5869, Time: 189.2210\n",
      "Epoch: 220, Loss: 181.7732, Train_Acc: 0.9231, TEST_Acc: 0.5847, Time: 190.0637\n",
      "Epoch: 221, Loss: 181.1072, Train_Acc: 0.9231, TEST_Acc: 0.5840, Time: 190.9082\n",
      "Epoch: 222, Loss: 177.1986, Train_Acc: 0.9231, TEST_Acc: 0.5789, Time: 191.7525\n",
      "Epoch: 223, Loss: 195.5679, Train_Acc: 0.8462, TEST_Acc: 0.5876, Time: 192.5962\n",
      "Epoch: 224, Loss: 182.4808, Train_Acc: 0.9231, TEST_Acc: 0.5804, Time: 193.4376\n",
      "Epoch: 225, Loss: 354.7364, Train_Acc: 0.6538, TEST_Acc: 0.6013, Time: 194.2740\n",
      "Epoch: 226, Loss: 716.2426, Train_Acc: 0.6154, TEST_Acc: 0.6208, Time: 195.1162\n",
      "Epoch: 227, Loss: 477.3601, Train_Acc: 0.6923, TEST_Acc: 0.6085, Time: 195.9603\n",
      "Epoch: 228, Loss: 641.9687, Train_Acc: 0.6154, TEST_Acc: 0.6251, Time: 196.8071\n",
      "Epoch: 229, Loss: 613.3394, Train_Acc: 0.6154, TEST_Acc: 0.6229, Time: 197.6530\n",
      "Epoch: 230, Loss: 796.5616, Train_Acc: 0.5962, TEST_Acc: 0.6323, Time: 198.4876\n",
      "Epoch: 231, Loss: 204.8123, Train_Acc: 0.9423, TEST_Acc: 0.5804, Time: 199.3217\n",
      "Epoch: 232, Loss: 212.4057, Train_Acc: 0.8846, TEST_Acc: 0.5407, Time: 200.1701\n",
      "Epoch: 233, Loss: 245.2029, Train_Acc: 0.8077, TEST_Acc: 0.5292, Time: 201.0153\n",
      "Epoch: 234, Loss: 225.7205, Train_Acc: 0.8462, TEST_Acc: 0.5263, Time: 201.8561\n",
      "Epoch: 235, Loss: 196.6075, Train_Acc: 0.8462, TEST_Acc: 0.5249, Time: 202.6982\n",
      "Epoch: 236, Loss: 210.8286, Train_Acc: 0.8462, TEST_Acc: 0.5213, Time: 203.5398\n",
      "Epoch: 237, Loss: 206.1655, Train_Acc: 0.8269, TEST_Acc: 0.5205, Time: 204.3883\n",
      "Epoch: 238, Loss: 197.9327, Train_Acc: 0.8462, TEST_Acc: 0.5191, Time: 205.2330\n",
      "Epoch: 239, Loss: 170.7982, Train_Acc: 0.8462, TEST_Acc: 0.5184, Time: 206.0797\n",
      "Epoch: 240, Loss: 110.6071, Train_Acc: 0.9038, TEST_Acc: 0.5234, Time: 206.9218\n",
      "Epoch: 241, Loss: 108.4513, Train_Acc: 0.9038, TEST_Acc: 0.5220, Time: 207.7667\n",
      "Epoch: 242, Loss: 106.5011, Train_Acc: 0.8846, TEST_Acc: 0.5205, Time: 208.6089\n",
      "Epoch: 243, Loss: 97.2985, Train_Acc: 0.9423, TEST_Acc: 0.5227, Time: 209.4509\n",
      "Epoch: 244, Loss: 100.6630, Train_Acc: 0.9423, TEST_Acc: 0.5263, Time: 210.2922\n",
      "Epoch: 245, Loss: 95.9878, Train_Acc: 0.9423, TEST_Acc: 0.5263, Time: 211.1322\n",
      "Epoch: 246, Loss: 97.1012, Train_Acc: 0.9423, TEST_Acc: 0.5270, Time: 211.9772\n",
      "Epoch: 247, Loss: 98.7627, Train_Acc: 0.9423, TEST_Acc: 0.5306, Time: 212.8124\n",
      "Epoch: 248, Loss: 90.9172, Train_Acc: 0.9423, TEST_Acc: 0.5263, Time: 213.6549\n",
      "Epoch: 249, Loss: 100.9270, Train_Acc: 0.9423, TEST_Acc: 0.5364, Time: 214.4937\n",
      "Epoch: 250, Loss: 97.0717, Train_Acc: 0.9423, TEST_Acc: 0.5335, Time: 215.3337\n",
      "Epoch: 251, Loss: 97.4914, Train_Acc: 0.9423, TEST_Acc: 0.5299, Time: 216.1794\n",
      "Epoch: 252, Loss: 102.7334, Train_Acc: 0.9423, TEST_Acc: 0.5342, Time: 217.0147\n",
      "Epoch: 253, Loss: 106.9742, Train_Acc: 0.9423, TEST_Acc: 0.5379, Time: 217.8497\n",
      "Epoch: 254, Loss: 92.8768, Train_Acc: 0.9423, TEST_Acc: 0.5321, Time: 218.6930\n",
      "Epoch: 255, Loss: 88.8064, Train_Acc: 0.9423, TEST_Acc: 0.5278, Time: 219.5362\n",
      "Epoch: 256, Loss: 89.1120, Train_Acc: 0.9423, TEST_Acc: 0.5285, Time: 220.3815\n",
      "Epoch: 257, Loss: 84.1143, Train_Acc: 0.9423, TEST_Acc: 0.5299, Time: 221.2261\n",
      "Epoch: 258, Loss: 83.6157, Train_Acc: 0.9423, TEST_Acc: 0.5314, Time: 222.0672\n",
      "Epoch: 259, Loss: 77.9921, Train_Acc: 0.9423, TEST_Acc: 0.5278, Time: 222.9062\n",
      "Epoch: 260, Loss: 79.4991, Train_Acc: 0.9231, TEST_Acc: 0.5220, Time: 223.7544\n",
      "Epoch: 261, Loss: 79.6306, Train_Acc: 0.9423, TEST_Acc: 0.5242, Time: 224.5997\n",
      "Epoch: 262, Loss: 76.9370, Train_Acc: 0.9231, TEST_Acc: 0.5249, Time: 225.4431\n",
      "Epoch: 263, Loss: 82.2096, Train_Acc: 0.9423, TEST_Acc: 0.5321, Time: 226.2795\n",
      "Epoch: 264, Loss: 76.7066, Train_Acc: 0.9423, TEST_Acc: 0.5364, Time: 227.1169\n",
      "Epoch: 265, Loss: 74.8921, Train_Acc: 0.9423, TEST_Acc: 0.5285, Time: 227.9648\n",
      "Epoch: 266, Loss: 77.6409, Train_Acc: 0.9423, TEST_Acc: 0.5278, Time: 228.8019\n",
      "Epoch: 267, Loss: 68.8957, Train_Acc: 0.9615, TEST_Acc: 0.5270, Time: 229.6451\n",
      "                  Testing accuracy: 0.5270, Time: 229.7184\n",
      "*****   END: 229.76674580574036  *****\n"
     ]
    }
   ],
   "source": [
    "# Breast Histology Images\n",
    "# Classify IDC vs non IDC images\n",
    "#\n",
    "# https://www.kaggle.com/simjeg/lymphoma-subtype-classification-fl-vs-cll\n",
    "#\n",
    "# This dataset consists of 5547 breast histology images of size 50 x 50 x 3,\n",
    "# The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images.\n",
    "#\n",
    "# Lenet 5\n",
    "#\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def print_time(message, start_time):\n",
    "    print(\"*****   \" + message + \": {}  *****\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "def get_batch(all_data, all_labels, batch_size=16):\n",
    "    # // means divide and result is integer, / returns float\n",
    "    # rows_data = len(all_data) // batch_size\n",
    "    # rows_labels = len(all_labels) // batch_size\n",
    "\n",
    "    rtn_data = all_data.reshape(-1, batch_size, int(50*50*3))\n",
    "    rnt_labels = all_labels.reshape(-1, batch_size, 2)\n",
    "\n",
    "    return (rtn_data, rnt_labels)\n",
    "\n",
    "\n",
    "def conv2d(x, weight, bias, strides=1):\n",
    "    x = tf.nn.conv2d(x, weight, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 50, 50, 3])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    print(\"conv1.get_shape(): \", conv1.get_shape())\n",
    "\n",
    "    # MAX POOLING\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(\"conv1.get_shape() after maxpool : \", conv1.get_shape())\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    print(\"conv2.get_shape(): \", conv2.get_shape())\n",
    "\n",
    "    # MAX POOLING\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(\"conv2.get_shape() after maxpool : \", conv2.get_shape())\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "   # fc1 = tf.nn.dropout(fc1, 0.75)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    # fc2 = tf.reshape(fc1, [-1, weights['wd2'].get_shape().as_list()[0]])\n",
    "    # fc2 = tf.add(tf.matmul(fc2, weights['wd2']), biases['bd2'])\n",
    "    # fc2 = tf.nn.relu(fc2)\n",
    "    # fc2 = tf.nn.dropout(fc2, 0.75)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "#\n",
    "#   Main\n",
    "#\n",
    "#\n",
    "######################################################################################################\n",
    "START_TIME = time.time()\n",
    "path = r'C:\\Users\\gtune\\OneDrive\\document_usk\\UCSC\\02_SecondQuater\\Deep_Learning_and_Artificial_Intelligence_with_TensorFlow\\Homework\\Final_Project\\data'\n",
    "\n",
    "X_data = np.load(\"X.npy\")\n",
    "Y_truth = np.load(\"Y.npy\")\n",
    "\n",
    "# print(X_data[0:10])\n",
    "# print(X_data.shape)\n",
    "# print(Y_truth[0:100])\n",
    "\n",
    "# print(np.array(np.where(Y_truth == 1)))\n",
    "\n",
    "train_data, test_data, train_Y, test_Y = \\\n",
    "    train_test_split(X_data, Y_truth, test_size=0.25, random_state=3)\n",
    "\n",
    "print(len(np.array(np.where(train_Y == 1)).ravel()))\n",
    "print(len(np.array(np.where(train_Y == 0)).ravel()))\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_Y.shape)\n",
    "#print(train_Y[0:50])\n",
    "\n",
    "# architecture hyper-parameter\n",
    "num_datapoints = len(train_data)\n",
    "learning_rate = 0.0001   # 0.001\n",
    "n_epoch = 1000\n",
    "batch_size = 52  # divisor of 4160 : 1, 2, 4, 5, 8, 10, 13, 16, 20, 26, 32, 40, 52, 64, 65, 80, 104, 130, 160, 208, 260, 320, 416, 520, 832, 1040, 2080, 4160\n",
    "\n",
    "n_input = 50*50*3\n",
    "n_classes = 2   # IDC and non-IDC\n",
    "dropout = 0.75\n",
    "\n",
    "# Normalization\n",
    "max_value = np.max(train_data)\n",
    "print('max_value -> {}'.format(max_value))\n",
    "\n",
    "train_data = np.array(train_data/max_value, dtype=np.float32)\n",
    "\n",
    "# data and Y(Label) reshaped\n",
    "onehot_train_Y = np.full((len(train_Y), n_classes), 0)\n",
    "onehot_train_Y[np.arange(0, len(train_Y)), train_Y] = 1\n",
    "train_data, onehot_train_Y = get_batch(train_data, onehot_train_Y, batch_size)\n",
    "\n",
    "test_data = test_data.reshape(-1, n_input)\n",
    "onehot_test_Y = np.full((len(test_Y), n_classes), 0)\n",
    "onehot_test_Y[np.arange(0, len(test_Y)), test_Y] = 1\n",
    "\n",
    "\n",
    "num_batches = num_datapoints // batch_size  # 10  # num_datapoints // batch_size\n",
    "\n",
    "print_time(\"num_batches\", START_TIME)\n",
    "\n",
    "# data_x, labels_y = get_batch(X_train, onehot_labels, batch_size)\n",
    "\n",
    "print(\"NO OF BATCHES:\", num_batches)\n",
    "print_time(\"NO OF BATCHES\", START_TIME)\n",
    "\n",
    "\n",
    "# tensorflow placeholder\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# wc 1,2  are filter\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 18])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 18, 48])),\n",
    "    'wd1': tf.Variable(tf.random_normal([13*13*48, 360])),\n",
    "    # 'wd2': tf.Variable(tf.random_normal([360, 252])),\n",
    "    'out': tf.Variable(tf.random_normal([360, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([18])),\n",
    "    'bc2': tf.Variable(tf.random_normal([48])),\n",
    "    'bd1': tf.Variable(tf.random_normal([360])),\n",
    "    # 'bd2': tf.Variable(tf.random_normal([252])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Create the Model\n",
    "model = conv_net(X, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate the model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initialing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorflow Session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(num_batches):\n",
    "            # for _ in range(1):\n",
    "\n",
    "            batch_x = train_data[i]\n",
    "            batch_y = onehot_train_Y[i]\n",
    "\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        # varidate after every epoch\n",
    "        batch_x = train_data[0]\n",
    "        batch_y = onehot_train_Y[0]\n",
    "\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:batch_x, Y:batch_y, keep_prob:1.0})\n",
    "\n",
    "        test_accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Train_Acc: %0.4f, TEST_Acc: %0.4f, Time: %0.4f\" % (epoch, losscalc, accuracycalc, test_accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "        # when train accuracy is over 95%, program end\n",
    "        if accuracycalc >= 0.95:\n",
    "            break\n",
    "\n",
    "    # display the accuracy of using testing data\n",
    "    accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "    print(\"                  Testing accuracy: %0.4f, Time: %0.4f\" % (accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "print_time(\"END\", START_TIME)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Untitled0.ipynb",
   "version": "0.3.2",
   "provenance": [
    {
     "file_id": "1B8mwfFJWeY9JRcfZEImsBnbJ2h3-m076",
     "timestamp": 1.553987425334E12
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
