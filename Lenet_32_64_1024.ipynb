{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_tJKMpm7rAdV",
    "colab_type": "code",
    "outputId": "694a0e56-9056-4a6e-8366-8f23a78529a0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553983635997E12,
     "user_tz": 420.0,
     "elapsed": 557438.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 79.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-189e9dfe-9014-4841-ae3a-499f81300ede\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-189e9dfe-9014-4841-ae3a-499f81300ede\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving X.npy to X (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sy-UJtg0tXYZ",
    "colab_type": "code",
    "outputId": "db5d03e5-c4cc-4347-bef4-13753c2bf065",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553984074708E12,
     "user_tz": 420.0,
     "elapsed": 8197.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 79.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-be637d22-370c-4e5e-b26b-eb0b8168ec73\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-be637d22-370c-4e5e-b26b-eb0b8168ec73\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Y.npy to Y (1).npy\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3UrfaZuKnfT8",
    "colab_type": "code",
    "outputId": "0c6dd942-e093-42b0-e530-13cc942d14f3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.553985042448E12,
     "user_tz": 420.0,
     "elapsed": 187609.0,
     "user": {
      "displayName": "yusuke yakuwa",
      "photoUrl": "",
      "userId": "15933669924163222944"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2550.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2087\n",
      "2073\n",
      "(4160, 50, 50, 3)\n",
      "(4160,)\n",
      "max_value -> 255\n",
      "*****   num_batches: 0.2625105381011963  *****\n",
      "NO OF BATCHES: 80\n",
      "*****   NO OF BATCHES: 0.2637145519256592  *****\n",
      "conv1.get_shape():  (?, 50, 50, 32)\n",
      "conv1.get_shape() after maxpool :  (?, 25, 25, 32)\n",
      "conv2.get_shape():  (?, 25, 25, 64)\n",
      "conv2.get_shape() after maxpool :  (?, 13, 13, 64)\n",
      "Epoch: 0, Loss: 8823.7979, Train_Acc: 0.7308, TEST_Acc: 0.5191, Time: 4.1430\n",
      "Epoch: 1, Loss: 9787.9326, Train_Acc: 0.7115, TEST_Acc: 0.5342, Time: 5.5006\n",
      "Epoch: 2, Loss: 9891.3945, Train_Acc: 0.7308, TEST_Acc: 0.5321, Time: 6.8665\n",
      "Epoch: 3, Loss: 9370.4561, Train_Acc: 0.7115, TEST_Acc: 0.5335, Time: 8.2213\n",
      "Epoch: 4, Loss: 9931.7920, Train_Acc: 0.7308, TEST_Acc: 0.5508, Time: 9.5781\n",
      "Epoch: 5, Loss: 8297.9395, Train_Acc: 0.7308, TEST_Acc: 0.5227, Time: 10.9398\n",
      "Epoch: 6, Loss: 7871.3389, Train_Acc: 0.7308, TEST_Acc: 0.5177, Time: 12.2934\n",
      "Epoch: 7, Loss: 8027.6753, Train_Acc: 0.7885, TEST_Acc: 0.5076, Time: 13.6477\n",
      "Epoch: 8, Loss: 7582.2466, Train_Acc: 0.7308, TEST_Acc: 0.5263, Time: 15.0100\n",
      "Epoch: 9, Loss: 7262.4043, Train_Acc: 0.7500, TEST_Acc: 0.5068, Time: 16.3674\n",
      "Epoch: 10, Loss: 7226.8120, Train_Acc: 0.7500, TEST_Acc: 0.5076, Time: 17.7206\n",
      "Epoch: 11, Loss: 6645.3311, Train_Acc: 0.7692, TEST_Acc: 0.5090, Time: 19.0759\n",
      "Epoch: 12, Loss: 6142.3091, Train_Acc: 0.7692, TEST_Acc: 0.5068, Time: 20.4313\n",
      "Epoch: 13, Loss: 5778.1870, Train_Acc: 0.8077, TEST_Acc: 0.5054, Time: 21.7882\n",
      "Epoch: 14, Loss: 5654.6221, Train_Acc: 0.7115, TEST_Acc: 0.5097, Time: 23.1459\n",
      "Epoch: 15, Loss: 5153.1279, Train_Acc: 0.7692, TEST_Acc: 0.5061, Time: 24.5062\n",
      "Epoch: 16, Loss: 4801.7529, Train_Acc: 0.8077, TEST_Acc: 0.5068, Time: 25.8597\n",
      "Epoch: 17, Loss: 4693.9858, Train_Acc: 0.8077, TEST_Acc: 0.5054, Time: 27.2204\n",
      "Epoch: 18, Loss: 4227.0337, Train_Acc: 0.7692, TEST_Acc: 0.5076, Time: 28.5884\n",
      "Epoch: 19, Loss: 3979.0198, Train_Acc: 0.7692, TEST_Acc: 0.5047, Time: 29.9545\n",
      "Epoch: 20, Loss: 4077.1045, Train_Acc: 0.7692, TEST_Acc: 0.5025, Time: 31.3187\n",
      "Epoch: 21, Loss: 3776.2495, Train_Acc: 0.7885, TEST_Acc: 0.5004, Time: 32.6803\n",
      "Epoch: 22, Loss: 3292.0405, Train_Acc: 0.7692, TEST_Acc: 0.5011, Time: 34.0360\n",
      "Epoch: 23, Loss: 3307.4978, Train_Acc: 0.7692, TEST_Acc: 0.5032, Time: 35.3933\n",
      "Epoch: 24, Loss: 3154.5173, Train_Acc: 0.7308, TEST_Acc: 0.5025, Time: 36.7591\n",
      "Epoch: 25, Loss: 2898.9988, Train_Acc: 0.7692, TEST_Acc: 0.5004, Time: 38.1096\n",
      "Epoch: 26, Loss: 2866.3481, Train_Acc: 0.7500, TEST_Acc: 0.5011, Time: 39.4680\n",
      "Epoch: 27, Loss: 2432.5129, Train_Acc: 0.7692, TEST_Acc: 0.4996, Time: 40.8279\n",
      "Epoch: 28, Loss: 2428.0759, Train_Acc: 0.7692, TEST_Acc: 0.5004, Time: 42.1794\n",
      "Epoch: 29, Loss: 2343.5935, Train_Acc: 0.7885, TEST_Acc: 0.5018, Time: 43.5360\n",
      "Epoch: 30, Loss: 2229.6033, Train_Acc: 0.7692, TEST_Acc: 0.5011, Time: 44.9013\n",
      "Epoch: 31, Loss: 1931.9491, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 46.2584\n",
      "Epoch: 32, Loss: 1803.2467, Train_Acc: 0.8077, TEST_Acc: 0.4996, Time: 47.6227\n",
      "Epoch: 33, Loss: 1912.3259, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 48.9895\n",
      "Epoch: 34, Loss: 1742.2830, Train_Acc: 0.7692, TEST_Acc: 0.4996, Time: 50.3416\n",
      "Epoch: 35, Loss: 1842.6523, Train_Acc: 0.7692, TEST_Acc: 0.4996, Time: 51.7016\n",
      "Epoch: 36, Loss: 1757.5887, Train_Acc: 0.7885, TEST_Acc: 0.4996, Time: 53.0602\n",
      "Epoch: 37, Loss: 1554.7115, Train_Acc: 0.7692, TEST_Acc: 0.4996, Time: 54.4149\n",
      "Epoch: 38, Loss: 1377.7443, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 55.7765\n",
      "Epoch: 39, Loss: 1234.2925, Train_Acc: 0.7885, TEST_Acc: 0.4996, Time: 57.1360\n",
      "Epoch: 40, Loss: 1171.1274, Train_Acc: 0.8462, TEST_Acc: 0.4996, Time: 58.5023\n",
      "Epoch: 41, Loss: 1266.5001, Train_Acc: 0.8654, TEST_Acc: 0.4996, Time: 59.8700\n",
      "Epoch: 42, Loss: 1172.4368, Train_Acc: 0.8654, TEST_Acc: 0.4996, Time: 61.2323\n",
      "Epoch: 43, Loss: 1091.5730, Train_Acc: 0.8462, TEST_Acc: 0.4996, Time: 62.5931\n",
      "Epoch: 44, Loss: 1060.9080, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 63.9469\n",
      "Epoch: 45, Loss: 976.6712, Train_Acc: 0.8269, TEST_Acc: 0.4989, Time: 65.3023\n",
      "Epoch: 46, Loss: 874.7559, Train_Acc: 0.8462, TEST_Acc: 0.4989, Time: 66.6750\n",
      "Epoch: 47, Loss: 1036.4529, Train_Acc: 0.8462, TEST_Acc: 0.4996, Time: 68.0290\n",
      "Epoch: 48, Loss: 893.7300, Train_Acc: 0.8269, TEST_Acc: 0.4989, Time: 69.3819\n",
      "Epoch: 49, Loss: 1059.2998, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 70.7444\n",
      "Epoch: 50, Loss: 990.5945, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 72.1013\n",
      "Epoch: 51, Loss: 937.7462, Train_Acc: 0.8462, TEST_Acc: 0.5004, Time: 73.4618\n",
      "Epoch: 52, Loss: 863.8051, Train_Acc: 0.8269, TEST_Acc: 0.5004, Time: 74.8244\n",
      "Epoch: 53, Loss: 879.9089, Train_Acc: 0.8462, TEST_Acc: 0.4996, Time: 76.1886\n",
      "Epoch: 54, Loss: 868.3627, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 77.5487\n",
      "Epoch: 55, Loss: 898.8497, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 78.9077\n",
      "Epoch: 56, Loss: 789.8313, Train_Acc: 0.8269, TEST_Acc: 0.5004, Time: 80.2668\n",
      "Epoch: 57, Loss: 842.4262, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 81.6256\n",
      "Epoch: 58, Loss: 812.1050, Train_Acc: 0.8269, TEST_Acc: 0.5004, Time: 82.9930\n",
      "Epoch: 59, Loss: 723.7120, Train_Acc: 0.8269, TEST_Acc: 0.5004, Time: 84.3513\n",
      "Epoch: 60, Loss: 718.7137, Train_Acc: 0.7885, TEST_Acc: 0.5004, Time: 85.7055\n",
      "Epoch: 61, Loss: 632.6172, Train_Acc: 0.7692, TEST_Acc: 0.5004, Time: 87.0710\n",
      "Epoch: 62, Loss: 530.6637, Train_Acc: 0.7692, TEST_Acc: 0.5004, Time: 88.4315\n",
      "Epoch: 63, Loss: 564.0181, Train_Acc: 0.7500, TEST_Acc: 0.5011, Time: 89.7974\n",
      "Epoch: 64, Loss: 504.6904, Train_Acc: 0.8077, TEST_Acc: 0.4996, Time: 91.1575\n",
      "Epoch: 65, Loss: 469.0618, Train_Acc: 0.7885, TEST_Acc: 0.4989, Time: 92.5217\n",
      "Epoch: 66, Loss: 479.7531, Train_Acc: 0.7692, TEST_Acc: 0.5025, Time: 93.8891\n",
      "Epoch: 67, Loss: 305.6609, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 95.2396\n",
      "Epoch: 68, Loss: 300.3459, Train_Acc: 0.8077, TEST_Acc: 0.5011, Time: 96.5948\n",
      "Epoch: 69, Loss: 372.2023, Train_Acc: 0.8077, TEST_Acc: 0.5018, Time: 97.9496\n",
      "Epoch: 70, Loss: 360.9985, Train_Acc: 0.7885, TEST_Acc: 0.5004, Time: 99.3091\n",
      "Epoch: 71, Loss: 360.5738, Train_Acc: 0.8654, TEST_Acc: 0.4996, Time: 100.6651\n",
      "Epoch: 72, Loss: 346.6015, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 102.0252\n",
      "Epoch: 73, Loss: 340.6759, Train_Acc: 0.8462, TEST_Acc: 0.4996, Time: 103.3856\n",
      "Epoch: 74, Loss: 361.4435, Train_Acc: 0.8462, TEST_Acc: 0.5011, Time: 104.7505\n",
      "Epoch: 75, Loss: 300.4766, Train_Acc: 0.8269, TEST_Acc: 0.4996, Time: 106.1137\n",
      "Epoch: 76, Loss: 315.4264, Train_Acc: 0.8462, TEST_Acc: 0.5011, Time: 107.4750\n",
      "Epoch: 77, Loss: 259.1883, Train_Acc: 0.8654, TEST_Acc: 0.5011, Time: 108.8373\n",
      "Epoch: 78, Loss: 277.6245, Train_Acc: 0.8654, TEST_Acc: 0.5004, Time: 110.1950\n",
      "Epoch: 79, Loss: 269.3350, Train_Acc: 0.8462, TEST_Acc: 0.5004, Time: 111.5534\n",
      "Epoch: 80, Loss: 357.5370, Train_Acc: 0.8462, TEST_Acc: 0.5011, Time: 112.9060\n",
      "Epoch: 81, Loss: 323.0394, Train_Acc: 0.8462, TEST_Acc: 0.5004, Time: 114.2623\n",
      "Epoch: 82, Loss: 250.2716, Train_Acc: 0.8654, TEST_Acc: 0.5032, Time: 115.6224\n",
      "Epoch: 83, Loss: 312.8670, Train_Acc: 0.8462, TEST_Acc: 0.5011, Time: 116.9809\n",
      "Epoch: 84, Loss: 293.1055, Train_Acc: 0.8462, TEST_Acc: 0.5018, Time: 118.3393\n",
      "Epoch: 85, Loss: 290.1146, Train_Acc: 0.8077, TEST_Acc: 0.5004, Time: 119.7012\n",
      "Epoch: 86, Loss: 231.4784, Train_Acc: 0.8077, TEST_Acc: 0.5018, Time: 121.0716\n",
      "Epoch: 87, Loss: 205.2788, Train_Acc: 0.8846, TEST_Acc: 0.5011, Time: 122.4395\n",
      "Epoch: 88, Loss: 219.3186, Train_Acc: 0.8654, TEST_Acc: 0.5047, Time: 123.7972\n",
      "Epoch: 89, Loss: 263.2300, Train_Acc: 0.8462, TEST_Acc: 0.5040, Time: 125.1515\n",
      "Epoch: 90, Loss: 207.7921, Train_Acc: 0.8077, TEST_Acc: 0.5068, Time: 126.5055\n",
      "Epoch: 91, Loss: 215.2067, Train_Acc: 0.8462, TEST_Acc: 0.5032, Time: 127.8613\n",
      "Epoch: 92, Loss: 203.4147, Train_Acc: 0.8462, TEST_Acc: 0.5018, Time: 129.2144\n",
      "Epoch: 93, Loss: 203.0504, Train_Acc: 0.8462, TEST_Acc: 0.5011, Time: 130.5723\n",
      "Epoch: 94, Loss: 150.9238, Train_Acc: 0.8462, TEST_Acc: 0.5025, Time: 131.9256\n",
      "Epoch: 95, Loss: 133.8808, Train_Acc: 0.8462, TEST_Acc: 0.5061, Time: 133.2856\n",
      "Epoch: 96, Loss: 127.0486, Train_Acc: 0.8654, TEST_Acc: 0.5054, Time: 134.6394\n",
      "Epoch: 97, Loss: 168.3890, Train_Acc: 0.8654, TEST_Acc: 0.5032, Time: 135.9974\n",
      "Epoch: 98, Loss: 180.3876, Train_Acc: 0.8654, TEST_Acc: 0.5061, Time: 137.3561\n",
      "Epoch: 99, Loss: 193.9294, Train_Acc: 0.8846, TEST_Acc: 0.5047, Time: 138.7171\n",
      "Epoch: 100, Loss: 179.8335, Train_Acc: 0.8846, TEST_Acc: 0.5061, Time: 140.0813\n",
      "Epoch: 101, Loss: 174.3782, Train_Acc: 0.8654, TEST_Acc: 0.5032, Time: 141.4466\n",
      "Epoch: 102, Loss: 159.8579, Train_Acc: 0.8269, TEST_Acc: 0.5032, Time: 142.8066\n",
      "Epoch: 103, Loss: 148.2079, Train_Acc: 0.8654, TEST_Acc: 0.5032, Time: 144.1615\n",
      "Epoch: 104, Loss: 139.6834, Train_Acc: 0.8846, TEST_Acc: 0.5032, Time: 145.5203\n",
      "Epoch: 105, Loss: 137.8641, Train_Acc: 0.8269, TEST_Acc: 0.5040, Time: 146.8748\n",
      "Epoch: 106, Loss: 156.8429, Train_Acc: 0.8462, TEST_Acc: 0.5054, Time: 148.2306\n",
      "Epoch: 107, Loss: 132.8761, Train_Acc: 0.8654, TEST_Acc: 0.5047, Time: 149.5880\n",
      "Epoch: 108, Loss: 161.9076, Train_Acc: 0.8462, TEST_Acc: 0.5047, Time: 150.9615\n",
      "Epoch: 109, Loss: 129.0327, Train_Acc: 0.8654, TEST_Acc: 0.5040, Time: 152.3237\n",
      "Epoch: 110, Loss: 115.0444, Train_Acc: 0.8269, TEST_Acc: 0.5054, Time: 153.6851\n",
      "Epoch: 111, Loss: 96.5551, Train_Acc: 0.8462, TEST_Acc: 0.5054, Time: 155.0387\n",
      "Epoch: 112, Loss: 65.4089, Train_Acc: 0.8269, TEST_Acc: 0.5054, Time: 156.3993\n",
      "Epoch: 113, Loss: 71.7660, Train_Acc: 0.8269, TEST_Acc: 0.5040, Time: 157.7498\n",
      "Epoch: 114, Loss: 82.2174, Train_Acc: 0.8654, TEST_Acc: 0.5076, Time: 159.1061\n",
      "Epoch: 115, Loss: 92.6682, Train_Acc: 0.8654, TEST_Acc: 0.5083, Time: 160.4643\n",
      "Epoch: 116, Loss: 59.7334, Train_Acc: 0.8654, TEST_Acc: 0.5083, Time: 161.8242\n",
      "Epoch: 117, Loss: 75.4041, Train_Acc: 0.9038, TEST_Acc: 0.5083, Time: 163.1805\n",
      "Epoch: 118, Loss: 42.3107, Train_Acc: 0.8846, TEST_Acc: 0.5061, Time: 164.5388\n",
      "Epoch: 119, Loss: 66.2402, Train_Acc: 0.7885, TEST_Acc: 0.5090, Time: 165.8947\n",
      "Epoch: 120, Loss: 43.6357, Train_Acc: 0.8846, TEST_Acc: 0.5068, Time: 167.2544\n",
      "Epoch: 121, Loss: 43.7011, Train_Acc: 0.8654, TEST_Acc: 0.5047, Time: 168.6251\n",
      "Epoch: 122, Loss: 38.8601, Train_Acc: 0.8654, TEST_Acc: 0.5068, Time: 169.9870\n",
      "Epoch: 123, Loss: 52.8811, Train_Acc: 0.9038, TEST_Acc: 0.5068, Time: 171.3499\n",
      "Epoch: 124, Loss: 44.3237, Train_Acc: 0.8846, TEST_Acc: 0.5083, Time: 172.7088\n",
      "Epoch: 125, Loss: 43.4110, Train_Acc: 0.8654, TEST_Acc: 0.5054, Time: 174.0604\n",
      "Epoch: 126, Loss: 48.4911, Train_Acc: 0.9038, TEST_Acc: 0.5061, Time: 175.4199\n",
      "Epoch: 127, Loss: 18.3511, Train_Acc: 0.9231, TEST_Acc: 0.5097, Time: 176.7794\n",
      "Epoch: 128, Loss: 10.4762, Train_Acc: 0.9038, TEST_Acc: 0.5040, Time: 178.1409\n",
      "Epoch: 129, Loss: 23.3887, Train_Acc: 0.9038, TEST_Acc: 0.5105, Time: 179.4982\n",
      "Epoch: 130, Loss: 23.7542, Train_Acc: 0.9231, TEST_Acc: 0.5090, Time: 180.8634\n",
      "Epoch: 131, Loss: 10.0524, Train_Acc: 0.9423, TEST_Acc: 0.5068, Time: 182.2245\n",
      "Epoch: 132, Loss: 6.6623, Train_Acc: 0.9423, TEST_Acc: 0.5076, Time: 183.5872\n",
      "Epoch: 133, Loss: 24.0686, Train_Acc: 0.9038, TEST_Acc: 0.5068, Time: 184.9504\n",
      "Epoch: 134, Loss: 2.0707, Train_Acc: 0.9808, TEST_Acc: 0.5097, Time: 186.3149\n",
      "                  Testing accuracy: 0.5097, Time: 186.4121\n",
      "*****   END: 186.44619464874268  *****\n"
     ]
    }
   ],
   "source": [
    "# Breast Histology Images\n",
    "# Classify IDC vs non IDC images\n",
    "#\n",
    "# LeNet\n",
    "#\n",
    "# Input: 50 x 50 x 3 (Channels)\n",
    "# Conv1: window:5x5, output:50 x 50 x 36\n",
    "# Pool1: strides[1,2,2,1],output:25 x 25 x 36\n",
    "# Conv1: window:5x5, output:50 x 50 x 36\n",
    "\n",
    "# https://www.kaggle.com/simjeg/lymphoma-subtype-classification-fl-vs-cll\n",
    "#\n",
    "# This dataset consists of 5547 breast histology images of size 50 x 50 x 3,\n",
    "# The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def print_time(message, start_time):\n",
    "    print(\"*****   \" + message + \": {}  *****\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "def get_batch(all_data, all_labels, batch_size=16):\n",
    "    # // means divide and result is integer, / returns float\n",
    "    # rows_data = len(all_data) // batch_size\n",
    "    # rows_labels = len(all_labels) // batch_size\n",
    "\n",
    "    rtn_data = all_data.reshape(-1, batch_size, int(50*50*3))\n",
    "    rnt_labels = all_labels.reshape(-1, batch_size, 2)\n",
    "\n",
    "    return (rtn_data, rnt_labels)\n",
    "\n",
    "\n",
    "def conv2d(x, weight, bias, strides=1):\n",
    "    x = tf.nn.conv2d(x, weight, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 50, 50, 3])\n",
    "\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "\n",
    "    print(\"conv1.get_shape(): \", conv1.get_shape())\n",
    "\n",
    "    # MAX POOLING\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    print(\"conv1.get_shape() after maxpool : \", conv1.get_shape())\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "\n",
    "    print(\"conv2.get_shape(): \", conv2.get_shape())\n",
    "\n",
    "    # MAX POOLING\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    print(\"conv2.get_shape() after maxpool : \", conv2.get_shape())\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "#\n",
    "#   Main\n",
    "#\n",
    "#\n",
    "######################################################################################################\n",
    "START_TIME = time.time()\n",
    "path = r'C:\\Users\\gtune\\OneDrive\\document_usk\\UCSC\\02_SecondQuater\\Deep_Learning_and_Artificial_Intelligence_with_TensorFlow\\Homework\\Final_Project\\data'\n",
    "\n",
    "X_data = np.load(\"X.npy\")\n",
    "Y_truth = np.load(\"Y.npy\")\n",
    "\n",
    "train_data, test_data, train_Y, test_Y = \\\n",
    "    train_test_split(X_data, Y_truth, test_size=0.25, random_state=0)\n",
    "\n",
    "print(len(np.array(np.where(train_Y == 1)).ravel()))\n",
    "print(len(np.array(np.where(train_Y == 0)).ravel()))\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_Y.shape)\n",
    "\n",
    "# architecture hyper-parameter\n",
    "num_datapoints = len(train_data)\n",
    "learning_rate = 0.0001\n",
    "n_epoch = 1000\n",
    "batch_size = 52  # divisor of 4160 : 1, 2, 4, 5, 8, 10, 13, 16, 20, 26, 32, 40, 52, 64, 65, 80, 104, 130, 160, 208, 260, 320, 416, 520, 832, 1040, 2080, 4160\n",
    "\n",
    "n_input = 50*50*3\n",
    "n_classes = 2   # IDC and non-IDC\n",
    "dropout = 0.75\n",
    "\n",
    "# Normalization\n",
    "max_value = np.max(train_data)\n",
    "print('max_value -> {}'.format(max_value))\n",
    "\n",
    "train_data = np.array(train_data/max_value, dtype=np.float32)\n",
    "\n",
    "# data and Y(Label) reshaped\n",
    "onehot_train_Y = np.full((len(train_Y), n_classes), 0)\n",
    "onehot_train_Y[np.arange(0, len(train_Y)), train_Y] = 1\n",
    "train_data, onehot_train_Y = get_batch(train_data, onehot_train_Y, batch_size)\n",
    "\n",
    "test_data = test_data.reshape(-1, n_input)\n",
    "onehot_test_Y = np.full((len(test_Y), n_classes), 0)\n",
    "onehot_test_Y[np.arange(0, len(test_Y)), test_Y] = 1\n",
    "\n",
    "\n",
    "num_batches = num_datapoints // batch_size  # 10  # num_datapoints // batch_size\n",
    "\n",
    "print_time(\"num_batches\", START_TIME)\n",
    "\n",
    "# data_x, labels_y = get_batch(X_train, onehot_labels, batch_size)\n",
    "\n",
    "print(\"NO OF BATCHES:\", num_batches)\n",
    "print_time(\"NO OF BATCHES\", START_TIME)\n",
    "\n",
    "\n",
    "\n",
    "# tensorflow placeholder\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# wc 1,2  are filter\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([13*13*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Create the Model\n",
    "model = conv_net(X, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate the model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initialing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorflow Session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(num_batches):\n",
    "            # for _ in range(1):\n",
    "\n",
    "            batch_x = train_data[i]\n",
    "            batch_y = onehot_train_Y[i]\n",
    "\n",
    "            # Use training data for optimization\n",
    "            sess.run(train_min, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        # varidate after every epoch\n",
    "        batch_x = train_data[0]\n",
    "        batch_y = onehot_train_Y[0]\n",
    "\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "\n",
    "        test_accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Train_Acc: %0.4f, TEST_Acc: %0.4f, Time: %0.4f\" % (\n",
    "        epoch, losscalc, accuracycalc, test_accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "        # when train accuracy is over 95%, program end\n",
    "        if accuracycalc >= 0.95:\n",
    "            break\n",
    "\n",
    "    # display the accuracy of using testing data\n",
    "    accuracycalc = sess.run(accuracy, feed_dict={X: test_data, Y: onehot_test_Y, keep_prob: 1.0})\n",
    "\n",
    "    print(\"                  Testing accuracy: %0.4f, Time: %0.4f\" % (accuracycalc, time.time() - START_TIME))\n",
    "\n",
    "print_time(\"END\", START_TIME)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Untitled0.ipynb",
   "version": "0.3.2",
   "provenance": [
    {
     "file_id": "1B8mwfFJWeY9JRcfZEImsBnbJ2h3-m076",
     "timestamp": 1.553985373957E12
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
